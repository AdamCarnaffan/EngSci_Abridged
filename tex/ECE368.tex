\documentclass[a4paper,12pt]{report}
\usepackage{color}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{listings}
\usepackage{media9}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{mathtools} 
\usepackage{amsmath}
\usepackage{extarrows} 

% \def\reals{{\rm I\!R}}
\def\reals{\mathbb{R}}
\def\integers{\mathbb{Z}}
\def\fft{\xlongleftrightarrow{\mathcal{F}}}
\def\fs{\xlongleftrightarrow{\mathcal{FS}}}


\newtheorem{theorem}{Theorem}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\begin{document}

\title{ECE368: Probabilistic Reasoning}
\author{Aman Bhargava}
\date{January-April 2021}
\maketitle

\tableofcontents

\section{Introduction and Course Information}

\paragraph{Course Information}
\begin{itemize}
\item Professors: Prof. Saeideh Parsaei Fard and Prof. Foad Sohrabi
\item Course: Engineering Science, Machine Intelligence Option
\item Term: 2021 Winter
\end{itemize}

\paragraph{Main Course Topics} 
\begin{itemize}
\item Vector, temporal, and spatial models.
\item Classification and regression model training.
\item Bayesian statistics, frequentist statistics.
\end{itemize}




\chapter{Review Topics}

\textit{See ECE286 notes for further reference} 

\section{Review of Probability Functions}

\paragraph{Probability Mass Function: } For \textit{discrete random variables}, $P_X(x)$denotes the probability that random variable $X$ takes on value $x$.

\paragraph{Probability Density Function: } For \textit{continuous random variables}, the probability $\Pr\{X\in [x_1, x_2]\}$ is given by $\int_{x_1}^{x_2} f_X(x) dx$.

Joint PMF's and PDF's are similarly defined. 

\paragraph{Marginal Probability Distributions: } Given joing PMF $P_{X, Y}(x, y)$ or PDF $f_{X,Y}(x, y)$, we can \textbf{marginalize} them as follows:
\begin{equation}
P_X(x) = \sum_{y\in Y}^{} P_{X,Y}(x, y)
\end{equation}
\begin{equation}
f_X(x) = \int_{-\infty}^{\infty} f_{X,Y}(x,y)dy
\end{equation}

\paragraph{Conditional Probability Functions: } 
\begin{equation}
P_{Y|X}(y, x) = \frac{P_{X,Y}(x,y)}{P_X(x)} 
\end{equation}


\paragraph{Prior Probability: } Probability \textbf{before} an additional observation is made (hence \textit{prior}). Example: $P_X(x)$.

\paragraph{Posterior Probability: } Probability \textbf{after} an observation is made (hence \textit{post}erior). Example: $P_{X|Y}(x | y)$.

\paragraph{Bayes Rule: }
\begin{equation}
P(B|A) = P(A|B)\frac{P(B)}{P(A)}
\end{equation}

\section{Expectation, Correlation, and Independence} 

\paragraph{Expectation Value: } $\mathbb E[x] = \sum_{x\in X}^{} P_X(x) = \int_{-\infty}^{\infty} xf_X(x) dx$

\paragraph{Law or Large Numbers: } $\lim_{N\to\infty} \frac{1}{N} \sum_{i=1}^{N} x_i = \mathbb E[X]$

\paragraph{Variance: } 

\begin{equation}
\begin{split}
\text{Var} (X) &= \mathbb E[(X-\mathbb E[x])^2] \\
&= \mathbb E[X^2] - (\mathbb E[X])^2
\end{split}
\end{equation}

\paragraph{Covariance: } 
\begin{equation}
\begin{split}
\text{Cov}(X,Y) &= \mathbb E[(X-\mathbb E[X]) (Y-\mathbb E[Y])] \\
&= \mathbb E_{XY}[XY] - \mathbb E[X]\mathbb E[Y]
\end{split}
\end{equation}

\paragraph{Correlation Coefficient: } 
\begin{equation}
\rho_{XY} = \frac{\text{Cov}(X,Y)}{\sqrt{\text{Var}(X)}\sqrt{\text{Var}(Y)}} 
\end{equation}
\begin{itemize}
\item $\rho_{XY}\in [-1, 1]$
\item $\rho > 0$ indicates positive correlation (line of best fit has positive slope).
\item $\rho < 0$ indicates negative correlation.
\item \textbf{$\mathbb E[XY] = \mathbb E[X] \mathbb E[Y]$ iff $X,Y$ are uncorrelated}.
\end{itemize}

\paragraph{Independence} 
\begin{theorem}{Independence}
Random variables $X,Y$ are independent \textbf{iff} 
\begin{equation}
P_{XY}(x,y) = P_X(x) \cdot P_Y(y)
\end{equation}
This also means that $\rho_{XY} = 0$, $P(X|Y) = P(X)$, etc.
\end{theorem}



\section{Laws of Large Numbers}

\paragraph{Weak Law: } Sample mean converges to the mean.

\paragraph{Strong Law: } If $\{x_i\}$ are \textbf{independent, identically distributed} (i.i.d.) random variables with mean $\mu$, then the \textbf{probability of} the sample mean = $\mu$ is 1 as $n\to \infty$.

\chapter{Parameter Estimation}

\section{Estimation Terminology}

\begin{itemize}
\item $\hat \theta_n$ is an \textbf{estimator} of some unknown parameter $\theta$.
\item \textbf{Estimation Error: } $\hat \theta_n - \theta$
\item \textbf{Bias} of estimator:  $\mathbb E[\hat \theta_n] - \theta$
\begin{itemize}
\item \textbf{Unbiased} estimator:  Bias$=0 = \mathbb E[\hat \theta_n] - \theta$.
\item \textbf{Asymptotically Unbiased: } $\lim_{n\to \infty} \mathbb E[\hat \theta_n] = \theta$ for all $\theta$.
\end{itemize}
\item \textbf{Consistency: } Estimator is consistent if $\lim_{n\to \infty} \hat \theta_n = \theta$.
\end{itemize}

\section{Maximum Likelihood Estimation}

\paragraph{Framing: } Let random variable $\vec X = [X_1, X_2, ..., X_n]$ be defined by either
\begin{enumerate}
\item Joint PMF $P_{\vec X}(\vec x; \theta)$
\item Joint PDF $f_{\vec X}(\vec x; \theta)$
\end{enumerate}

$\vec x$ is a series of measurements. 

\paragraph{Maximum Likelihood Estimation: } The ML estimate of model parameter $\theta$ is
\begin{equation}
\hat \theta_n = \text{argmax}_\theta P_{\vec X}(\vec x; \theta)
\end{equation}

\paragraph{Independent, identically distributed case: } If each $x_i\in\vec x$ are independent and identically distributed, then

\begin{equation}
P_{\vec X}(\vec x; \theta) = \prod_{i=1}^n P_{X}(x_i; \theta)
\end{equation}

Which we can convert to a summation by taking the \textbf{log-likelihood} (recall that logarithm is monotonically increasing, so maximizing log-likelihood is equialent to maximizing likelihood).

\begin{equation}
\hat\theta_n = \text{arg}\max_{\theta} (\sum_{i=1}^{n} \log P_X(x_i; \theta))
\end{equation}


\section{Frequentist vs. Bayesian Statistics}

\paragraph{Frequentist: } In \textbf{classical statistics}, probability is taken to be approximately equal to the \textbf{frequency of events}. Model parameters are assumed to have some deterministic, fixed value (even though they might be unknown).

\paragraph{Bayesian Statistics: } Model parameters are treated as \textbf{random variables} with their own distributions.
\begin{itemize}
\item Generally the more modern approach.
\item We are most interested in the \textbf{joint probability distribution} of model parameters and model arguments (e.g., $f_x(x, \theta)$).
\item \textbf{Main criticism:} probabilities are assigned to unrepeatable events (arguably violates the definition of probability as the limit of event frequency).
\end{itemize}


\section{Maximum a Posteri Estimation (MAP)}

\begin{equation}
\begin{split}
\hat\theta_{map} &= \text{arg}\max_{\theta} f_{\theta | x} (\theta | x)\\
&= \text{arg}\max_\theta f_{X|\theta} (x | \theta) \frac{f_\theta(\theta)}{f_X(x)} 
\end{split}
\end{equation} 


Where $f_\theta(\theta)$ is the \textbf{prior distribution} of model parameter. 
\begin{itemize}
\item If $f_\theta(\theta)$ is uniform, we will still get the same answer as a \textbf{maximum likelihood} estimation.
\end{itemize}


\subsection{Picking a Prior Distribution}

\paragraph{Best Practice: } Pick a distribution of the same form as $f_{X|\theta}(x|\theta)$ (called ``conjugate pair'').

\paragraph{Beta Distribution: } Used for \textbf{binomial distribution}.
\begin{itemize}
\item Binomial distribution: 
\begin{equation}
P_{X = k | \theta} = {n\choose k} \theta^k(1-\theta)^{n-k}
\end{equation}
where 
\begin{itemize}
\item $\theta$: Probability of success on each Bernoulli trial.
\item $n$: Total number of trials.
\item $k$: Total number of successful trials.
\end{itemize}
\item \textbf{Beta Distribution: } 
\begin{equation}
f_\theta(\theta; \alpha, \beta) = \begin{cases}
c\theta^{\alpha-1}(1-\theta)^{\beta-1} & \text{for } \theta\in [0,1] \\
0 & \text{else}
\end{cases}
\end{equation}

Where
\begin{itemize}
\item $\alpha, \beta$ are customizable parameters.
\item $c = [\Gamma(\alpha + \beta)]/[\Gamma(\alpha)\Gamma(\beta)]$
\item $\Gamma(x) \equiv \int_{0}^{\infty} u^{x-1}e^{-u} du$
\item $\Gamma(x+1) = x\Gamma(x)$ for all $x\in \mathbb R$.
\item $\Gamma(n+1) = n!$ for integer $n$.
\item $\therefore c = \frac{(\alpha+\beta-1)!}{(\alpha-1)!(\beta-1)!}$ for integer $\alpha, \beta$.
\item $\mu_f = \mathbb E[f_\theta(\theta)] = \frac{\alpha}{\alpha + \beta}$
\item Maximum likelihood $\text{arg}\max_\theta f_\theta(\theta) = \frac{\alpha-1}{\alpha+\beta-2}$
\end{itemize}
\end{itemize}


\section{Conditional Expectation Estimator}

\paragraph{Key Idea: } Find the \textbf{expected value} for the estimator given your observations.

\begin{equation}
\hat\theta_{conditional expectation} = \mathbb E[\theta | \vec X = \vec x] = \int_{-\infty}^\infty \theta f_{\theta|\vec x}(\theta | \vec x)
\end{equation}


\section{Bayesian Least Mean Square Estimator (LMS)}

\paragraph{Key Idea: } To estimate random variable model parameter $\theta$, we find

\begin{equation}
\hat \theta_{LMS} = \text{arg}\min_{\hat \theta} \mathbb E [(\theta-\hat\theta)^2]
\end{equation}

\begin{itemize}
\item $\hat \theta_{LMS} = \mathbb E[\theta]$ achieves the goal.
\item \textbf{Equivalently: } We can also find 
\begin{equation}
\hat\theta_{LMS} = \text{arg}\min_{\hat \theta} (\mathbb E[\theta-\hat\theta])^2
\end{equation}
\end{itemize}


\chapter{Hypothesis Testing}

\paragraph{Goal: } Given two hypotheses $H_0, H_1$ and observation $\pmb x = (x_0, \dots, x_n)$, we wish to decide which hypothesis is \textbf{better}.

\paragraph{Error Types: } These are generally with respect to $H_0$, or the ``null hypothesis''.
\begin{itemize}
\item \textbf{Type I:} False \textbf{rejection}. We reject hypothesis $H_0$ despite it being the correct one.
\item \textbf{Type II:} False \textbf{acceptance}. We accept hypothesis $H_0$ despite it being false.
\end{itemize}


\section{Likelihood Ratio Test}

\begin{equation}
\mathbb L(\pmb x) = \frac{P_x(\pmb x; H_1)}{P_x(\pmb x; H_0)} \lessgtr z
\end{equation}

If $\mathbb L(\pmb x) > z$, we \textbf{reject $H_0$}. Else, we \textbf{fail to reject} $H_0$. $z = 1$ corresponds to the maximum likelihood decision rule.

\paragraph{Neyman-Pearson Lemma: } We let $\alpha$ represent the probability of \textbf{false rejection} of $H_0$ and $\beta$ represent the probability of \textbf{false acceptance} of $H_0$ (i.e., type I and type II error probability respectively).

\begin{equation}
P(\mathbb L(\pmb x) > z ; H_0) = \alpha
\end{equation}
\begin{equation}
P(\mathbb L(\pmb x) \leq z ; H_1) = \beta
\end{equation}

To be completely honest, I don't know why this lemma deserves a name. In any case, there is a direct tradeoff between $\alpha$ and $\beta$ -- they are inversely proportional (i.e., we cannot get better overall confidence ``for free'' with the same data). There is also equivalence between selecting some likelihood cutoff $z$ and some $\gamma$ for $\pmb x \lessgtr \gamma$. 

\section{Bayesian Hypothesis Testing}

The likelihood ratio test is essentially a \textbf{maximum likelihood} method. Bayesian hypothesis testing is equivalent to \textbf{maximum a posteri} methods.

\begin{itemize}
\item \textbf{Goal: } Choose the most probable hypothesis \textit{given the data}.
\item \textbf{Given: } Hypotheses $\pmb \theta = \{\theta_1, \dots, \theta_M\}$, data $\pmb x = (x_1, \dots, x_n)$.
\item \textbf{Method: } Select the optimal hypothesis based on 
\begin{equation}
\begin{split}
\hat \theta &= \arg\max_{\theta\in \pmb \theta} P(\theta | \pmb x) \\
&= \arg\max_{\theta\in \pmb \theta} P(\pmb x | \theta)P(\theta)
\end{split}
\end{equation}
\end{itemize}

By maintaining $\pmb x$ as a free variable in these computations, the rejector regions $\mathcal R$ for $\pmb x$ can be found relatively easily.


\section{Gaussian Vector Distribution}

\paragraph{Scalar Gaussian Normal Distribution: } 
\begin{equation}
\begin{split}
f_x(x) &= \frac{1}{\sqrt{2\pi\sigma^2}} \exp[\frac{-(x-\mu)^2}{2\sigma^2}] \\
&\Leftrightarrow x \sim \mathcal N(\mu, \sigma^2)
\end{split}
\end{equation}


\paragraph{Gaussian Vector} 
\begin{equation}
f_{\vec x}(\vec x; \vec \mu, \Sigma) = \frac{1}{(2\pi)^{D/2}} \frac{1}{|\Sigma|^{\frac{1}{2}}} \exp [\frac{-1}{2} (\vec x - \vec \mu)^T \Sigma^{-1} (\vec x - \vec \mu)]
\end{equation}
Where 
\begin{itemize}
\item $D$ is the dimension of the vectors $x$.
\item $\vec \mu\in \mathbb R^D$ is the mean vector.
\begin{itemize}
\item $\vec \mu = \mathbb E[\vec x]$
\end{itemize}

\item $\Sigma \in \mathbb R^{D\times D}$ is the covariance matrix. It is \textbf{positive semidefinite}.
\begin{itemize}
\item $\Sigma = \mathbb E [(\vec x - \vec \mu)(\vec x - \vec \mu)^T]$
\end{itemize}


\end{itemize}

\subsection{Eigen Analysis of Gaussian Vectors}

\textit{From ECE367} : All PSD matrices $A$ have \textbf{orthogonal} eigenvectors. We can also arbitrarily scale them to be ortho\textbf{normal}. 
\begin{equation}
A = Q\Lambda Q^T
\end{equation}

Where each column of $Q$ is an eigenvector and $\Lambda = \text{diag}(\text{eigen values})$. $QQ^T = I$ and $Q^T = Q^{-1}$ by orthonormalcy. 

\paragraph{Applying to Gaussian Vectors: } 
Take a gander at the term $\frac{-1}{2} (\vec x - \vec \mu)^T \Sigma^{-1} (\vec x - \vec \mu)$. Since $\Sigma$ is PSD we can decompose it into 
\begin{equation}
\begin{split}
\Sigma &= Q \Lambda Q^T \\
\Sigma^{-1} &= Q \Lambda^{-1} Q^T \\
\end{split}
\end{equation}

We define a helper variable $\vec y$ as follows:

\begin{equation}
\begin{split}
& \vec y \equiv Q^T(\vec x - \vec \mu) \\
& \Rightarrow \vec y^T = (\vec x - \vec \mu)^T Q \\
& \Rightarrow (\vec x - \vec \mu)^T \Sigma^{-1} (\vec x - \vec \mu) = \vec y^T Q^T \Sigma^{-1} Q \vec y \\
&= \vec y^T \Lambda^{-1} \vec y^T
\end{split}
\end{equation}

The big payoff is that $\vec y$ is a random variable with a \textbf{diagonal} covariance matrix (i.e., independent components). $\vec y$ also has zero mean! What a swell result.

\begin{equation}
\begin{split}
f_y(\vec y) &= \frac{1}{(2\pi)^{D/2} |\Lambda|^{1/2}} \exp (\frac{-1}{2} \vec y^T \Lambda^T \vec y) \\
&= \prod_{i=1}^D \frac{1}{\sqrt{2\pi\lambda_i}} \exp[\frac{-y_i^2}{2\lambda_i}]
\end{split}
\end{equation}


\section{Gaussian Estimation}

\paragraph{Given: } Observations $x_i\in \mathbb R$ where we know that were generated by $x_i = \theta + w_i$ and $w_i \sim \mathcal N(0, \sigma_i^2)$. In other words, each $x_i$ is a ``measure'' of the same mean value $\theta$, but there is some zero-mean noise $w_i$ with variance $\sigma_i^2$. Note that each data point has its own variance. 

\paragraph{Goal: } Estimate $\theta$.

\subsection{Maximum Likelihood}

\begin{equation}
\begin{split}
\hat \theta_{ML} &= \arg\max_{\theta} f_x(\vec x ; \theta) \\
&= \arg\max_\theta \prod_{i= 1}^n \frac{1}{\sqrt{2\pi} \sigma_i} \exp[\frac{-1(x-\theta)^2}{2\sigma_i^2}] \\
\hat \theta_{ML} &= \frac{\sum_{i=1}^{n} \frac{x_i}{\sigma_i^2}}{\sum_{i=1}^{n} \frac{1}{\sigma_i^2}}
\end{split}
\end{equation}

Intuitively, this corresponds to a weighted sum of each $x_i$. Weight is proportional to $\frac{1 }{\sigma_i^2}$, which corresponds to ``certainty'' in the validity of the data point in representing $\theta$.

\subsection{MAP Estimation}

We assume a \textbf{conjugate prior} distribution for $\theta \sim \mathcal N(x_0, \sigma_0^2)$. Conveniently enough, these $x_0, \sigma_0$ are functionally identical to just having another data point!

\begin{equation}
\begin{split}
\hat \theta_{MAP} &= \arg\max_{\theta} f_{\theta}(\theta) f(x | theta) \\
&= \dots \\
\hat \theta_{MAP} &= \frac{\sum_{i=0}^{n} \frac{x_i}{\sigma_i^2}}{\sum_{i=0}^{n} \frac{1}{\sigma_i^2}} 
\end{split}
\end{equation}

\textbf{Note that the sums now start at zero to incorporate the prior distribution's information!} 

% TODO: First part of March 1 content (match filtering!).


\chapter{Statistical Machine Learning}

\section{Naive Bayesian Classifier}

\textit{Technically this was in the first half, but it fits well to motivate LDA and QDA.} 

\paragraph{Goal: } Let $\theta\in \{1,2\}$ be the class of data points $\vec x \in \mathbb R^n$. We wish to find $P(\theta | \vec x)$.

\paragraph{Naive Bayesian Assumption: } Each component of $\vec x$ is \textbf{independent} with respect to class $\theta$. By the probability axioms relating to independence, 
\begin{equation}
P_{\vec x | \theta}(\vec x | \theta) = \prod_{i = 1}^n P_{x_i | \theta}(x_i | \theta)
\end{equation}


\paragraph{Classification Equations: } 
\begin{equation}
P(\theta | \vec x) = \frac{P(\theta) \prod_{i=1}^n P_{x_i | \theta}(x_i | \theta) }{P(\vec x)} 
\end{equation}
Where $P(\vec x) = \sum_{\theta}^{} [P(\theta) \prod_{i=1}^n P_{x_i|\theta} (x_i | \theta)]$

\begin{equation}
P(\theta = 1) \prod_{i=1}^n P_{x_i | \theta}(x_i | \theta = 1) \lessgtr P(\theta = 2) \prod_{i=1}^n P_{x_i | \theta}(x | \theta = 2)
\end{equation}


\paragraph{Bag of words: } For classifying text, we choose a set of $W$ of the most common words. In our vectors $\vec x$, each index $x_i$ corresponds to whether word $w_i$ appears in the given text. 

\begin{equation}
P_{x_i | \theta}(x_i = w_d | \theta = j) = \frac{\text{occurrences of } w_i \text{ in set }j}{\text{total words in set }j} 
\end{equation}


\paragraph{Laplace Smoothing: } Even if we perform the computation in the log domain, we might run into a $P_{x_i|theta}(x_i | \theta) = 0$ if we lack a datapoint (word occurrence) for a given class. Laplace smoothing simply appends the vocabulary set $W$ to each class (i.e., all words in the set occur at least once a priori).




\section{Linear Discriminant Analysis (LDA)}

\paragraph{Goal: } Create algorithm $\text{LDA}: \vec x_i \to c_i$ where $\vec x_i$ is a data point and $c_i\in C$ is the class it belongs to.

\paragraph{General Methodology: } We use \textbf{Bayesian hypothesis testing} with variable classes modelled with normal distributions. An important simplifying assumption is that each class $c$ has different $\vec \mu_c$ \textbf{BUT} they all have the same $\Sigma$. This is what makes our decision boundaries \textbf{linear} (hence ``LDA'').

\paragraph{Classification Equations: } 
\begin{enumerate}
\item $\hat y(\vec x) = \arg\max_{c\in C} \beta_c^T \vec x + \gamma_c$ where
\begin{itemize}
\item $\beta_c = \Sigma^{-1} \mu_c$
\item $\pi_c = P(c)$ (prior probability of class $c$)
\item $\gamma_c = \log(\pi_c) - \frac{1}{2} \mu_c^T \Sigma^{-1} \mu_c$
\end{itemize}

\item Posterior probability of class $c$: 
\begin{equation}
P(c | \vec x) = \frac{\exp(\beta_c^T x + \gamma_c)}{\sum_{c'\in C}^{} \exp(\beta_{c'}^T x + \gamma_{c'})} 
\end{equation}
\end{enumerate}


\section{Quadratic Discriminant Analysis (QDA)}

\paragraph{Goal: } Same as LDA. 
\paragraph{Assumptions: } We are given $\mu_c, \Sigma_c, \pi_c$ for each class, and each $\vec x_i \sim \mathcal N(\mu_c, \Sigma_c)$ for some $c\in C$. Note the different $\Sigma_c$ for each class.

\paragraph{Classification Equations: } 
\begin{equation}
\hat y(\vec x) = \arg\max_{c\in C} [\log (\tilde \pi_c) - \frac{1}{2} (x-\mu_c)^T \Sigma^{-1}_c (x-\mu_c)]
\end{equation}
Where $\tilde \pi_c = \frac{1}{(2\pi)^{D/2} |\Sigma_c|^{1/2}} \pi_i$

\begin{equation}
P(c | x) = \pi_c \frac{1}{(2\pi)^{D/2} |\Sigma_c|^{1/2} } \exp(\frac{-1}{2} (x-\mu_c)^T \Sigma^{-1} (x - \mu_c))\frac{1}{P(x)} 
\end{equation}
Where $P(x)$ is given by
\begin{equation}
\begin{split}
P(x) &= \sum_{c'\in C}^{} P(x | c')P(c') \\
&= \sum_{c'\in C}^{} \mathcal N(x; \mu_c, \Sigma_c) P(c')
\end{split}
\end{equation}

\paragraph{Calculating $\mu_c, \Sigma_c$} 

The maximum likelihood estimation of these are as follows: 
\begin{equation}
\hat\mu_c = \frac{1}{n} \sum_{i=1}^{n} x_i \,\, \forall x_i \in c
\end{equation}
\begin{equation}
\hat \Sigma_c^{ML} = \frac{1}{n} \sum_{i=1}^{n} (x_i - \hat \mu_c)(x_i - \hat\mu_c)^T\,\, \forall x_i \in c
\end{equation}
\textbf{PROBLEM:} $\hat \Sigma_c^{ML}$ is a biased estimator. For the $D=1$ case, $\mathbb E[\hat \sigma_c^{ML}] = \frac{n-1}{n} \sigma^2$. To correct, we use the following:
\begin{equation}
\hat \Sigma_c^{corrected} = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \hat \mu_c)(x_i - \hat\mu_c)^T\,\, \forall x_i \in c
\end{equation}

The primary problem with these sorts of classifiers is that $\Sigma$ has high dimensionality. If we use the same $\Sigma$ for all classes, we get LDA. If we force $\Sigma_c$ to be diagonal, we get a naive Bayesian classifier.





% TODO: March 1 lecture videos notes.

\section{General Bayesian Inference on Gaussian Vectors}

\paragraph{Given: } We know that $\vec z \sim \mathcal N(\vec \mu, \Sigma)$. We know the value of only part of the $\vec z$ vector, and want to determine the probability distribution and ML/MLE/MMSE estimation for the remaining indices we don't know. 

\begin{equation}
\vec z = \begin{bmatrix}
\vec x \\
\vec y \\
\end{bmatrix}
\end{equation}

We are given the value of $\vec y$ and want to know $\vec x$.

\paragraph{Key Tools: } 
\begin{equation}
\Sigma = \begin{bmatrix}
\Sigma_{xx} & \Sigma_{xy} \\
\Sigma_{yx} & \Sigma_{yy}
\end{bmatrix}
\end{equation}
Where $\Sigma_{xy} = \mathbb E[(x - \mu_x)(x - \mu_x)^T]$.

\begin{itemize}
\item \textit{The maximum of a Gaussian is at the mean!} 
\item $f_{x|y} \sim \mathcal N$
\item $\hat x_{MAP} = \hat x_{MMSE,LMS} = \mathbb E[\vec x | \vec y]$
\end{itemize}


\paragraph{Results: } 

\begin{equation}
f_{x|y}(x|y) \sim \mathcal N(\mu_{x|y}, \Sigma_{x|y})
\end{equation}
Where 
\begin{equation}
\mu_{x|y} = \mu_x + \Sigma_{xy} \Sigma_{yy}^{-1} (y-\mu_y)
\end{equation}
\begin{equation}
\Sigma_{x|y} = \Sigma_{xx} - \Sigma_{xy}\Sigma^{-1}_{yy}\Sigma_{yx}
\end{equation}

Which is pretty cool, especially considering that $\hat x_{MAP, MMSE} = \mu_{x|y}$ is now linear with $\vec y$. We can think of $\Sigma_{x|y}$ as the ``remaining uncertainty'' in $\vec x$ after we receive information $\vec y$. 


\section{Linear Gaussian Systems}

\paragraph{Given: } $P(x) \sim \mathcal N(\mu_x, \Sigma_x)$. $y = Ax+b+z$ where
\begin{itemize}
\item $A$ is a known matrix.
\item $b$ is a known vector.
\item $z\sim \mathcal N(0, \Sigma_z)$
\item $x,z$ are independent.
\end{itemize}

\paragraph{Goal: } We observe $y$. What are our estimates $\hat x_{MAP}$, $\hat x_{MMSE}$ that correspond to this particular $y$?

\paragraph{Solution: } Unsurprisingly, $x,y$ have Gaussian distributions.
\begin{equation}
\begin{split}
\hat x_{MAP, MMSE} &= \mu_x + \Sigma_{xy}\Sigma_{yy}^{-1}(y-\mu_y) \\
\hat x_{MAP, MMSE} &= (\Sigma_x^{-1} + A^T \Sigma_z^{-1} A)^{-1} (A^T \Sigma_z^{-1}(y-b) + \Sigma_x^{-1}\mu_x)
\end{split}
\end{equation}
Where
\begin{equation}
\mu_y = A\mu_x + b
\end{equation}
\begin{equation}
\Sigma_{xy} = \Sigma_x A^T
\end{equation}
\begin{equation}
\Sigma_{yy} = A\Sigma_xA^T + \Sigma_z
\end{equation}
\begin{equation}
\Sigma_{x|y} = (\Sigma_x^{-1} + A^T\Sigma_z^{-1}A)^{-1}
\end{equation}
\begin{equation}
\mu_{x | y} = \Sigma_{x|y}(A^T \Sigma_z^{-1}(y-b) + \Sigma_x^{-1}\mu_x)
\end{equation}




\chapter{Linear Regression}

\paragraph{Linear Regression Model: } Our aim is to predict $y$ as a linear combination of indices of vector $\vec x$. A key difference now is that we are looking at \textbf{probabilistic} least squares, so we want a measure of certainty about our predictions. This is why we introduce a $\sigma^2$ term:
\begin{equation}
p(y, \vec x, \theta) = \mathcal N(y | \vec w^T\vec x, \sigma^2)
\end{equation}
And of course $\mathbb E[y|\vec x] = \vec w^T \vec x$. You can also apply polynomial, gaussian, and sigmoid basis functions to transform $\vec x$ to get a more complex decision boundary.

\paragraph{Some Notation: } 
\begin{itemize}
\item $\mathcal D = (\vec y, X)$.
\item $\theta = (\vec w, \sigma^2)$
\end{itemize}
Note that from here on out, we use \textbf{augmented vectors} in $X$. That is, we have a $1$ at the 1st index of each $X$ to account for the affine offset for each. This way, we don't need to include messy ``bias'' terms.

$$\vec x_{augmented} = \begin{bmatrix} 1\\\vec x \end{bmatrix}$$




\section{Ordinary Least Sqauares (MLE) Linear Regression}

Generic linear regression (no regularization, no prior) tries to find $\hat \theta_{MLE}$ as follows: 

\begin{equation}
\begin{split}
\hat \theta_{MLE} &= \arg\max_{\theta} \log[p(\mathcal D | \theta)] \\
&= \arg\max_{\theta} \frac{-1}{2\sigma^2} \sum_{i=1}^{N} (y_i - \vec w^T \vec x_i)^2 - \frac{N}{2} \log(2\pi\sigma^2)
\end{split}
\end{equation}

\paragraph{Solutions for Ordinary Least Squares} 

\begin{equation}
\vec w_{OLS} = (X^T X)^{-1} X^T y
\end{equation}
\begin{equation}
\sigma^2_{ML} = \frac{1}{N} \sum_{n=1}^{N} [y_n - \vec w^T \vec x_n]^2
\end{equation}

Interestingly, least squares is identical to projecting $\vec y$ onto the span of the columns of dataset $X$ under the $\ell_2$ norm. $\vec w$ holds the projection coefficients for this. 


\section{Regularized Least Squares}

\paragraph{Key idea: } We introduce a penalization term in our error related to parameter $\vec w$. $\text{Error} = E_{\mathcal D}(\vec w) + \lambda E_{\vec w}(\vec w)$. Known as ``weight decay'', ``parameter shrinkage''. 

\paragraph{Ridge Regression: } Literally just $\ell_2$ regularized least squares. 

\begin{equation}
\begin{split}
\vec w_{ridge} &= (X^TX + \lambda I )^{-1} X^T y \\
&= (\tilde X^T\tilde X)^{-1} \tilde X^T \tilde y
\end{split}
\end{equation}
Where $$\tilde X = \begin{bmatrix} X\\-I\lambda \end{bmatrix}$$ $$\tilde y = \begin{bmatrix} y\\0\\\vdots\\0 \end{bmatrix}$$



Interestingly, this is identical to the MAP estimate of $\vec w$ with prior distribution $p(\vec w) = \prod_{j=1}^d \mathcal N(w_j | 0, \tau^2)$ where $\lambda^2 = \sigma^2/\tau^2$:

\begin{equation}
\begin{split}
\hat w_{MAP} &= \arg\max_{\vec w} \log P(\mathcal D | \vec w, \sigma)P(\vec w) \\
&= \arg\min_{\vec w} \sum_{i=1}^{n} (y_i - \vec w^T \vec x_i)^2 + \lambda^2 ||\vec w||_2^2
\end{split}
\end{equation}


You can also apply previously discussed linear Gaussian vector formulations by introducing noise variable $\vec z \sim \mathcal N(\vec 0, \sigma^2 I)$ and using $\vec w$ as the variable we are trying to estimate with prior $\vec w \sim \mathcal N (\vec 0, \tau^2 I)$. 

\section{Logistic Regression}

Here we apply a sigmoid function to the $\vec w^T \vec x$ value and use the resulting value as the probability of ``positive'' $y$ via a bernoulli distribution:

\begin{equation}
P(y | \vec w, \vec x) = \text{Bernoulli}(\sigma(\vec w^T \vec x))
\end{equation}

\begin{equation}
\hat w_{ML} = \max_{\vec w}[-\sum_{i=1}^{n} y_i\log(\sigma(\vec w^T\vec x)) + (1-y_i) \log(1-\sigma(\vec w^T \vec x))  ]
\end{equation}

This is literally cross entropy loss. There's no analytical solution, so we need to use numerical techniques (see ECE421). It's also pretty easy to add regularization to this ``loss'' function.

\section{Bayesian Linear Regression}

In case you wanted to be really extra and compute the full probability distributions of $\vec w$, $\sigma^2$ for a linear regression model, you can use Bayesian linear regression.

\paragraph{Goal: } We wish to compute \textbf{full posteriors} for $\vec w, \sigma^2$ given the dataset $\mathcal D$. We assume a Gaussian prior on $\vec w$ so that the end distribution is also nice and Gaussian:

\paragraph{Tools to Get There: } I won't build the full proof, but here are the building blocks:
\begin{equation}
p(\vec w) = \mathcal N(\vec w | \vec w_0, \Sigma_0)
\end{equation}

\begin{equation}
p(\vec y | X, \vec w, \sigma^2) = \mathcal N(\vec y | X\vec w, \sigma^2I_{N\times N})
\end{equation}

\paragraph{Solution for Known $\sigma$: } Using ``Bayes theorem for Gaussians'',
\begin{equation}
\begin{split}
p(\vec w | X, \vec y, \sigma^2) &\propto \mathcal N(\vec w | \vec w_0, \Sigma_0)\mathcal N(\vec y | X\vec w, \sigma^2 I_{N\times N}) \\
&= \mathcal N(\vec w | \vec w_N, \Sigma_N)
\end{split}
\end{equation}
Where 
\begin{equation}
\begin{split}
\vec w_N &= \Sigma_N\Sigma_0^{-1}\vec w_0 + \frac{1}{\sigma^2} \Sigma_N X^T\vec y \\
\Sigma_N^{-1} &= \Sigma_0^{-1} + \frac{1}{\sigma^2} X^T X
\end{split}
\end{equation}



\paragraph{Some notes on intuition: } 
\begin{itemize}
\item Since $\Sigma_N^{-1}$ scales with $X^T X$, it will grow very large as the number of data points increases. This means that $\Sigma_N$ will be pretty small with many samples, which intuiviely makes sense because additional data (should) result in more ``certainty''.
\item For incorporating a non-Gaussian prior, see the Murphy textbook section on Bayesian linear regression.
\end{itemize}

\paragraph{Posterior Distribution of $y$: } Let's say we are given some dataset $\mathcal D$ to train on. Now we want to perform ``inference'' on some new data point $\vec x$ to get a distribution on the corresponding value of $y$: 
\begin{equation}
p(y | \vec x, \mathcal D, \sigma^2) = \mathcal N(y | \vec w_N^T \vec x, \sigma^2_N(x))
\end{equation}
Where $\sigma_N^2(x) = \sigma^2 + \vec x^T\Sigma_N\vec x$.


\paragraph{Bayesian Regression with Unknown $\sigma^2$: } Generally, we are not going to know what $\sigma^2$ is in advance for $y$. See Murphy p. 234 for how to do this (this topic was not covered in this course).






\chapter{Markov Chains}

Often we are interested in understanding the evolution of some random variable over time (e.g., a robot's position). Here, we examing \textbf{Markov decision processes}, a framework for understanding the evolution of some random variable over discrete time steps and discrete states. \textbf{Markov chains} are of particular interest.

\section{Preliminary Definitions}

\paragraph{Markov Chain: } Consists of 
\begin{enumerate}
\item A finite set of discrete \textbf{states} $S = \{1, \dots, m\}$.
\begin{itemize}
\item At each time step $n$, our random variable is in one of those state $X_n = s_i \in S$.
\end{itemize}
\item \textbf{Transition probabilities:} $\Pr(i\to j) \forall i,j\in S$. Matrix $A\in \mathbb R^{m\times m}$ as 
\begin{equation}
A_{i,j} = \Pr(X_{n+1} = j | X_n = i)
\end{equation}

\item \textbf{Memoryless assumption:} The probability distribution of the next state depends \textit{only} on the current state. Also known as the ``Markov property''.
\item There can be no other states than those in $S$, so $\sum_{j=1}^{m} A_{i,j} = 1$ (rows of $P$ sum to 1).
\end{enumerate}

\paragraph{Transition probability matrix: } Matrix $A$ from above always has an eigen value of $1$, and the rows all sum to $1$. It's called a \textit{stochastic matrix}, and we learned about these in ECE367! It can also be represented as a transition probability graph with edge weights representing the probability of traversal.

\paragraph{Path probability: } $P(\langle X_0 \to X_1 \to \dots \to X_n\rangle) = \prod_{i=1}^{n-1} A_{X_i \to X_{i+1}}$.

\paragraph{Initial conditions: } We can have a \textit{probabilistic initial condition} with vector $\vec \pi^0$:
\begin{equation}
\Pr(X_0 = j) = \vec \pi_j^{(0)}
\end{equation}
We can find the probability vector for the next time step as:
\begin{equation}
\vec \pi^{(i+1)} = \vec\pi^{(i)T}A
\end{equation}


\section{Markov Chain Steady State}

\paragraph{Definition: } If $\lim_{n\to \infty} \vec \pi^{(n)} = \tilde \pi$ regardless of the starting state $\vec \pi_j^{(0)}$, then $\tilde \pi$ is the \textbf{steady state distribution} satisfying
\begin{equation}
\tilde \pi = \tilde \pi^T A
\end{equation}

This has deep connections to flow optimization (ECE358: Algorithms and Datastructures). When viewed on the graphical model of the chain, the steady state distribution corresponds to the ``flow'' into each node being equivalent to the ``flow'' out of each node. 

\paragraph{Conditions for Steady State: } 
\begin{enumerate}
\item A \textbf{periodic} Markov chain has no steady state. The steady state behaviour of these chains oscillate between two or more nodes with $\Pr = 1$ (think of a 2-node cycle with transition probabilities of $1$).
\item \textbf{Multiple recurrent graph} Markov chains have no steady state. This occurs when some node has an edge configuration such that, after traversing the node, part of the graph can never be reached. An example of this is if the starting node has ONLY two outgoing nodes leading to disconnected sub-graphs. 
\item \textbf{Absorbing states} yield Markov chains with no steady state. If a state has transition probability $1$ for returning to itself, then it ``absorbs'' all paths into itself forever. 
\item \textbf{Singly connected/``Recurrent States''} are a REQUIREMENT for a steady state to exist. There must exist a path from $i\to j$ and $j\to i$ for all $i,j\in S$.
\end{enumerate}

\begin{theorem}{Steady State Convergence Theorem: }
For all Markov chains with 
\begin{enumerate}
\item Finite states
\item Single recurrent classes
\item Aperiodicity
\end{enumerate}

There exists a \textbf{steady state distribution} $\tilde \pi$ such that:
\begin{enumerate}
\item $\lim_{n\to \infty} \pi_j^{(n)} = \tilde \pi_j$ for all initial states.

\item $\tilde \pi$ is a unique solution to
\begin{equation}
\begin{split}
\tilde\pi &= \tilde\pi^T P \\
\sum_{j=1}^{m} \tilde\pi_j &= 1
\end{split}
\end{equation}

\item For all ``transition states'' $j$, $\tilde\pi_j = 0$.
\item For all ``recurrent states'' $i$, $\tilde\pi_i > 0$.
\end{enumerate}
\end{theorem}


\subsection{Eigen Analysis of Steady State}

Recall the steady state equation: 
\begin{equation}
\tilde \pi^T A = \tilde\pi
\end{equation}

Let $M = A^T$ and $\vec x = \tilde\pi^T$. We can frame this as an eigen value problem as follows with $\lambda = 1$:

\begin{equation}
M\vec x = \lambda \vec x
\end{equation}

\begin{theorem}{}
For stochastic matrix $M$, there exists eigen value $\lambda = 1$, and the rest of the eigen values satisfy $|\lambda| < 1$.


Since $\det(A) = \det(A^T)$, $M$ and $A$ have the same eigen values. We can also show that iterated multiplications by $A$ result in all eigenvectors except the one corresponding to $\lambda = 1$ go to zero. Therefore, the steady state value will be the eigenvector associated with $\lambda =1$.
\end{theorem}

\paragraph{Power iteration algorithm: } To find steady state (i.e., $\vec v^{(1)}$): 

\begin{enumerate}
\item Initialize $v^{(0)}$.
\item For $\ell = 1, \dots$:
\begin{equation}
v^{(\ell)} = \frac{Mv^{(\ell-1)}}{||Av^{(\ell-1)}||} 
\end{equation}
\item End.
\end{enumerate}

Applications include Pagerank (see ECE367.pdf).




\chapter{Directed Graphical Models}

Diagramatic representations of joint and conditional probability distributions can be very helpful for representing and understanding the relationships between random variables. For graphical models, we have:
\begin{itemize}
\item \textbf{Nodes:} Random variables.
\item \textbf{Edges:} Joint/conditional distribution connections.
\end{itemize}

Directed and undirected graphs are the main classes for graphical models. Directed graphical models are known as ``Bayes nets'', ``causal networks'', or ``belief networks''. Undirected models are known as ``random Markov fields''.


\section{Defining Graphical Models}

\paragraph{Hierarchy: } If there exists an edge connecting node $a\to b$, then $a$ is a parent of $b$ and $b$ is a child of $a$. This also means that there exists some term $p(b|a)$ in the joint probability distribution corresponding to the entire graph. That is, the value of $a$ informs the value of $b$.

\paragraph{Graphical Model $\to$ Joint Probability Distribution: } For a graph composed of nodes $x_{1:N}$, the joint probability distribution is:
\begin{equation}
P(x_{1:N}) = \prod_{i=1}^N P(x_i | pa_i)
\end{equation}
Where $pa_i$ is the set of \textbf{parents} of node $x_i$.


\paragraph{Independence: } Recall that independence between $A,B$ occurs when knowing the value of $B$ does not inform the probability distribution of $A$ (and vice versa):
\begin{equation}
P(A|B) = P(A); \,\,\, P(B|A) = P(B)
\end{equation}
\begin{equation}
P(A,B)= P(A)P(B)
\end{equation}
\paragraph{Conditional Independence: } Independence when the two variables are conditioned on some tertiary variable $C$. Surprisingly, conditional independence generally does not imply regular independence.
\begin{equation}
P(A,B|C) = P(A|C)P(B|C)
\end{equation}

Which is equivalent to 

\newcommand\ci{\perp\!\!\!\perp}

\begin{equation}
A \ci B \perp C
\end{equation}

Generally, we arrive at conditional independence by comparing the values of $P(A|C)P(B|C)$ with $P(A,B|C) = P(A,B,C)/P(C)$.


\section{Conditional Independence for Connection Types}

\paragraph{Head to Head Connections: } A head to head connection is when the ``arrow heads'' of our graphical model meet at the same node. For a graphical model of $a,b,c$, we would have edges $a\to c$ and $b\to c$ forming a \textit{head to head} connection at $c$.


\paragraph{Head to Tail Connection: } $c$ has a head-to-tail connection if we have $a\to c$ and $c\to b$. 

\paragraph{Tail to Tail Connections: } $c$ has a tail-to-tail connection if we have $c\to a$ and $c \to b$.


\begin{theorem}{Connection Types and their Conditional Independences: }
\begin{itemize}
\item \textbf{Head-to-Head:} $a$ is independent of $b$. They are NOT conditionally independent given $c$, though!
\begin{equation}
a \ci b \text{ BUT } a \not\ci b \perp c
\end{equation}

\item \textbf{Head-to-Tail:} $a$ is independent of $b$ given $c$. This corresponds to a Markov chain's next state depending only on the present state. 
\begin{equation}
a \ci b \perp c
\end{equation}

\item \textbf{Tail-to-Tail:} $a$ and $b$ are generally dependent on eachother. However, once you know the shared parent, they become \textit{independent}.
\begin{equation}
a \not \ci b \text{ BUT } a \ci b \perp c
\end{equation}
\end{itemize}
\end{theorem}



\section{Direct Separation (D-Sep)}

D-separation property for directed graphs tells us if we have conditional independence for subsets of a graph.

\begin{theorem}{D-separation theorem: }
Let $A,B,C$ be disjoint subsets of the nodes in a directed, acyclic graphical model.  To ascertain whether $A \ci B \perp C$, we do the following:
\begin{enumerate}
\item Consider all paths $p$ from all nodes in $A$ to all nodes in $B$ (note that this path can traverse any edge regardless of the direction of the edge).
\item A path is \textbf{blocked} if it includes a node $c$ where either
\begin{enumerate}
\item There is a \textbf{head-to-tail} or \textbf{tail-to-tail} meeting some node $c\in C$.
\item There is a \textbf{head-to-head} meeting at node $c\notin C$ AND none of the \textbf{descendants} of $c$ are in $C$.
\end{enumerate}

If all paths $p$ are blocked, then $A$ is d-separated from $B$ by $C$. All joint distributions over all variables satisfy $A \ci B \perp C$.
\end{enumerate}
\end{theorem}

\paragraph{Utility of D-separation: } We can determine how/if variables and parameters in a model are conditionally independent.


\section{Markov Blanket/Boundary}

\paragraph{Definition: } A markov blanket for a given node $x_i$ consists of the 
\begin{itemize}
\item Parents
\item Children
\item Co-parents
\end{itemize}
of the node. The meaning is that $P(x_i|x_{\{j\neq i\}}) = f(\text{Markov blanket nodes})$. That is, you only really ``need'' the nodes in the Markov blanket to maximally ``inform'' the distribution of variable $x_i$.


This can be shown by analysis of $p(x_i|x_{i\neq j})$:

\begin{equation}
\begin{split}
p(x_i | x_{j\neq i}) &= \frac{p(x_{1:N})}{\int p(x_{1:N}) dx_i} \\
&= \frac{\prod_k p(x_k | \text{pa}_k)}{\int \prod_k p(x_k | \text{pa}_k) dx_i}
\end{split}
\end{equation}

and factoring out terms in the integral in the denominator that are not functions of $x_i$ (i.e., those not members of the Markov blanket). As one might expect, the node $x_i$ is independent from the rest of the graph given the Markov blanket node values.




\chapter{Markov Random Fields (Undirected Graphical Models)}

\section{Conditional Independence Properties}

The analogy for D-separation is simpler for undirected graphical models since we don't need to worry about the case of descendants informing us, and there is no way to distinguish head-to-head vs. head-to-tail connections.

\begin{theorem}{Conditional Independence for Undirected Graphical Models: }
Let $A,B,C$ be disjoint subsets of the nodes in the graph. $A\ci B \perp C$ \textbf{iff} \textit{all paths} from any node in $A$ to any node in $B$ \textbf{pass through} $C$.
\end{theorem}

From here, we can derive that the \textbf{Markov blanket} for any node is simply \textbf{all directly neighbouring nodes}.


\section{Factorizing a Markov Random Field}

\paragraph{Maximal Clique: } A subset of the nodes in $G$ that is a clique BUT would cease to be a clique if any more nodes were added. 

We factorize undirected graphical models in terms of the max cliques in the graph.

\begin{itemize}
\item Let $x_c$ be set of variables in max clique $c$.
\item Let $\phi_c()$ be a \textbf{potential} function on clique $x_c$ where $\phi_c(x_c) \geq 0$ always.
\end{itemize}
Then we have
\begin{equation}
p(\pmb x) = \frac{1}{Z} \prod_c \phi_c(x_c)
\end{equation}

That is, we multiply the potential functions on all the max cliques in $\pmb x$ and normalize with factor $Z$ to get the joint probability distribution of $\pmb x = \{x_1, \dots, x_N\}$.

\paragraph{Energy Functions: } To obtain potential functions $\phi_c \geq 0$, we can let $\phi_c(x_c) = \exp[-E(x_c)]$ where $E(x_c)$ is the energy function. Note that potential functions $\phi$ and energy functions $E$ have no true probabilistic meaning. They simply denote ``preferable'' configurations for the system. 

\paragraph{Graphical Models as Filters: } 
\begin{itemize}
\item Let $UI = \{p(\pmb x) | p(\pmb x) \text{ aligns with conditional distributions represented in the graph}\}$.
\item Let $UF = \{p(\pmb x) | p(\pmb x) \text{ that can be expressed as } \frac{1}{Z} \prod_c \phi_c(x_c)\}$.
\end{itemize}

\textbf{Theorem: } $UI = UF$. We can regard the graphical model as a ``filter'' that only ``lets through'' distributions that satisfy the aforementioned requirements on the conditionals.

\section{Relating Undirected Graphical Models to Directed}

The general method for conversion from directed to undirected graphical models is as follows:
\begin{enumerate}
\item For head-to-head connections $p(x_i | x_j, x_k, x_\ell)$, we \textbf{marry the parents} by adding additional undirected connections between them.
\item The rest of the directed links are converted to undirected edges.
\item We initialize $\phi_c(x_c) = 1$ for all max cliques $c$ in the graph.
\item We multiply in the corresponding terms from the directed version to each $\phi_c$ such that $p(\pmb x)$ remains the same.
\end{enumerate}


\paragraph{D-map: } A graph id a d-map of $p(\pmb x)$ if all conditional independence statements on $p(\pmb x)$ are reflected in the graph.
\paragraph{I-map: } A graph is an i-map of $p(\pmb x)$ if all conditional independence statements implied by $G$ are atisfied by $p(\pmb x)$.


A perfect graph is a d-map and an i-map.



\chapter{Inference on Graphical Models}

Graphical models offer a convenient way to understand the \textbf{reasoning} behind predictions. Here we examine some efficient methods that take advantage of graph structure. For marginalizing inference, we would generally need to do the following:

\begin{equation}
P(x_n) = \sum_{x_1}^{} \sum_{x_2}^{} \dots \sum_{x_N}^{} p(x_1, \dots, x_N)
\end{equation}

These nested sums make for pretty terrible run time ($O(N^k)$ where each $x_i$ has domain of size $k$), and would need to be replaced by ugly integrations for continuous variable.

\paragraph{General strategy -- ``pushing in'' sums: } To make this process faster, we the following technique. For a factorized probability distribution $p(x_1, \dots, x_N)$, we can ``push in'' summation operators as follows:
\begin{equation}
\sum_{i=1}^{n} \alpha f(i) = \alpha \sum_{i=1}^{n} f(i)
\end{equation}

Where $\alpha$ can be any terms that are not functions of $i$, the summation variable. In other words, if factorized elements are not a function of the summation variable, they can be factored out to the left of the corresponding summation operator. Since graphical models have a strong ability to factorize probability distributions effectively based on graph structure, we can leverage this for faster inference algorithms.

\section{Message Passing for First Order Markov Chain}

For a first order Markov chain $x_1 \to x_2 \to \dots \to x_N$, we can convert to an undirected model and find the following joint PDF on all nodes:

\begin{equation}
P(\pmb x) = \frac{1}{Z} \phi_{1,2}(x_1, x_2) \cdot \phi_{2,3}(x_2, x_3) \cdot \dots \cdot \phi_{N-1,N}(x_{N-1}, x_N)
\end{equation}

The ``message passing'' algorithm will take advantage of this ``pushing in'' strategy to arrive at the following:

\begin{equation}
\begin{split}
p(x_n) &= \frac{1}{Z} [\sum_{x_{n-1}}^{} \phi_{n-1, n} (x_{n-1}, x_n) \dots [ \sum_{x_2}^{} \phi_{2,3}(x_2, x_3) [\sum_{x_1}^{} \phi_{1,2}(x_1, x_2) ] ]  ] \\
& \cdot [\sum_{x_{n+1}}^{} \phi_{n, n+1}(x_n, x_{n+1}) \dots [\sum_{x_N}^{} \phi_{N-1, N}(x_{N-1}, x_N)] ]\dots] ] ]
\end{split}
\end{equation}

\begin{equation}
p(x_n) = \frac{1}{Z } \mu_\alpha(x_n) \mu_\beta(x_n)
\end{equation}

Note the recursive structure of $\mu_\alpha,\mu_\beta$.
\begin{equation}
\begin{split}
\mu_{\alpha}(x_n) &= \sum_{x_{n-1}}^{} \phi_{n-1, n}(x_{n-1}, x_n) \cdot \mu_\alpha(x_{n-1}) \\
\mu_{\beta}(x_n) &= \sum_{x_{n+1}}^{} \phi_{n+1, n} (x_{n+1}, x_n) \cdot \mu_{\beta}(x_{n+1}) \\
\end{split}
\end{equation}

With base cases
\begin{equation}
\begin{split}
\mu_{\alpha}(x_2) &= \sum_{x_1}^{} \phi_{1,2}(x_1, x_2) \\
\mu_\beta(x_{N-1}) &= \sum_{x_N}^{} \phi_{N-1, N}(x_{N-1}, x_N)
\end{split}
\end{equation}


The time complexity is now only $O(NK^2)$. This is a very efficient method, in part because dynamic programming can also be used to calculate $\mu_\alpha, \mu_\beta$. These $\mu$ values correspond to ``forward'' message and ``backward'' messages respectively in the chain.


\section{Message Passing for Inference on Trees}

A similar scheme to above can be employed for fast inference on tree graphical models. 

\paragraph{Polytree: } If the tree structure has nodes with more than 1 parent. However, there must be only one path between each pair of nodes (disregarding edge direction). 

\subsection{Factor Graphs}

A factor graph is another method for visualizing graphical models that we will make use of for the message passing algorithm on trees. 
\begin{itemize} 
\item Two node types: Circles for random variable nodes, squares for \textbf{factor functions}.
\item \textbf{Factor functions} represent elements of the factorization for the joint probability function. They connect to each random variable node that is an argument for the particular factor function.
\item Note that an undirected graph can be easily converted by introducing one \textbf{factor node} per max clique $c$. 
\item To convert directed graphs into factor graphs, we first convert to undirected graphs. 
\end{itemize}

Factor graphs are \textbf{bipartite} because members of the \textit{factor function} node class only connect with members of the \textit{random variable} node class.

\subsection{Generalized Message Passing Algorithm}

\paragraph{Input: } Any undirected tree, directed tree, polytree.

\paragraph{Output: } A marginalized joint PDF $P(x_n)$

\begin{equation}
\begin{split}
p(x_n) &= \sum_{\forall x\in \pmb x | x\neq x_n}^{} p(\pmb x) \\
&= \prod_{s\in ne(x)} [\sum_{X_s}^{} F_s(x, X_s)] \\
&= \prod_{s\in ne(x)} [\sum_{X_s} \mu_{f_s\to x}]
\end{split}
\end{equation}

Where $ne(x)$ is the set of factor nodes neighbouring $x$, $X_s$ is the set of variables in subtree at factor node $f_s$. $F_s(x, X_s)$ is the product of all the factor nodes in the subtree rooted at $f_s$ (i.e., the probability factor corresponding to the entire subtree at $f_s$). 


\begin{enumerate}
\item Convert to factor graph.
\item Initialize each $\mu_{x\to f} = 1$ where $x\in \pmb x$ and $f$ is a factor node connected to $x$.
\item Initialize each $\mu_{f\to x} = f(x)$ where $f(x)$ is the factor function.

\item Begin by computing variable $\to$ factor node messages:
\begin{equation}
\mu_{x\to f}(x) = \prod_{\ell \in Ne(x) | \ell \neq f} \mu_{f_l \to x}(x)
\end{equation}

\item Next, calculate factor $\to$ variable node messages:
\begin{equation}
\mu_{f_s\to x_i}(x_i) = \sum_{\pmb x_s | \neq x_i}^{} \phi_{f_s}(\pmb x) \cdot \mu_{x_1 \to f_s} \cdot \mu_{x_2 \to f_s} \cdot \dots \cdot \mu_{x_N \to f_s}
\end{equation}
Where $x_{1:N}$ are the children of factor node $f_s$.

\item Return \begin{equation}
f(x_n) = \frac{1}{Z} \prod_{s\in ne(x)} \mu_{f_s t\ x}(x)
\end{equation}
\end{enumerate}

This algorithm gives an exact solution for inference and can approximate solutions for graphs with cycles. The max-product algorithm uses similar techniques to solve for $\pmb x^* = \arg\max P(\pmb x)$.

\subsection{Max Sum Algorithm Sketch}

\begin{enumerate}
\item Initialize 
\begin{equation}
\begin{split}
\mu_{x\to f}(x) &= 0 \\
\mu_{f\to x}(x) &= \ln f(x)
\end{split}
\end{equation}

\item Recursion: 
\begin{equation}
\begin{split}
\mu_{f\to x}(x) &= \max_{\pmb x = ne(f)}[\ln(f(\pmb x)) + \sum_{m\in ne(f) | m \neq x}^{} \mu_{x\to f} (x) ] \\
\mu_{x\to f}(x) &= \sum_{\ell \in ne(x) | \ell \neq f}^{} \mu_{f_\ell \to x}(x)
\end{split}
\end{equation}
\end{enumerate}




\chapter{Hidden Markov Models}

\section{Introduction}

A hidden Markov model is a Markov process with indirect observation of the state. Standard Markovian assumptions still hold, namely that the future depends only on the present state.

\paragraph{Notation: } 
\begin{itemize}
\item Hidden states are represented as $z_n$ for $n\in [N]$. The full sequence is denoted $\pmb z$.
\item Observations or ``emissions'' are denoted $x_n$ for $n\in [N]$. The full sequence is denoted $\pmb x$.
\item We are generally given emission distribution $P(x | z)$ and transition probabilities $P(z_{n+1} | z_n)$.
\end{itemize}

Our goal is usually about inferring hidden state from observations. The two sub-problems are (1) estimating a specific hidden state $p(z_n)$ and (2) finding the most likely sequence of states.

\begin{equation}
P(\pmb x | \pmb z) = \prod_{i = 1}^N P(x_i | z_i)
\end{equation}
\begin{equation}
P(\pmb x, \pmb z) = p(z_1) (\prod_{i = 2}^N P(z_n | z_{n-1})) (\prod_{n=1}^N P(x_n | z_n))
\end{equation}


\paragraph{General Procedures for Estimating $\hat z_n$: } Given $\pmb x$, we can use MLE
\begin{equation}
\hat z_n = \arg\max_{z_n} P(z_n)
\end{equation}
Where $P(\pmb z, \pmb x)$ is given as above. We simply must marginalize \textbf{efficiently} using the message passing algorithm. When used with HMM's, this is known as the \textbf{forward backward algorithm}.

\section{Forward-Backward Algorithm}

Let us convert the directed Markov chain to $h - z_1 - f_2 - f_3 - \dots - f_{n-1} - z_{n-1} - f_n - z_n - \dots - z_n$ where 
\begin{equation}
h(z_1) = P(z_1)P(x_1 | z_1)
\end{equation}

and each $z_i$ represents a hidden node and each $f_i$ represents a factor node. Forward messages are represented as $\alpha_n$ for $n\in [N]$ and backward messages are $\beta_n$.

\begin{equation}
\begin{split}
\alpha(z_{n-1}) &= \mu_{z_{n-1}\to f_n}(z_{n-1}) = \mu_{f_{n-1} \to z_{n-1}}(z_{n-1}) \\
&= P(x_n | z_n) \sum_{z_{n-1}}^{} P(z_n | z_{n-1}) \alpha(z_{n-1})
\end{split}
\end{equation}

\begin{equation}
\begin{split}
\beta(z_n) &= \mu_{f_{n+1}\to z_{n}}(z_n) \\
&= \sum_{z_{n+1}}^{} \beta(z_{n+1})P(x_{n+1} | z_{n+1}) P(z_{n+1} | z_n)
\end{split}
\end{equation}


These are pretty much the same as our $\mu_{\alpha,\beta}$ values from the message passing before.

\begin{equation}
\begin{split}
P(z_n, \pmb x) &= \alpha(z_n) \beta(z_n) \\
P(z_n | \pmb x) &= \frac{\alpha(z_n)\beta(z_n)}{P(\pmb x)} \\
&= \gamma(z_n)
\end{split}
\end{equation}

\textbf{Intuition for $\alpha, \beta, \gamma$:} Bases can be inferred from these as well.
\begin{itemize}
\item $\alpha(z_n) = P(z_n, x_{1:n})$. Hence we initialize $\alpha(z_1) = P(z_1, x_1)$.
\item $\beta(z_n) = P( x_{(n+1):(N)} | z_n)$. Hence we initialize $\beta(z_N) = 1$ because $x_{N+1:N}$ doesn't really exist.
\item $\gamma (z_n) = P(z_n | x_{1:N})$
\end{itemize}


\subsection{Implementing Forward-Backward Algorithm}

$\alpha, \beta$ can become very small since they are conditioned on $x_{1:n}, x_{n+1:N}$ respectively. Therefore, we introduce the following:

\begin{equation}
\begin{split}
\hat \alpha(z_n) &= \frac{\alpha(z_n)}{P(x_{1:n})} = P(z_n | x_{1:n}) \\
\hat beta &= \frac{\beta(z_n)}{P(x_{n+1:N})} \\
\end{split}
\end{equation}

Importantly, this corresponds to $\alpha(z_n), \beta(z_n)$ being normalized!

\paragraph{Recursion Formulae: } To convert from $\alpha, \beta \to \hat\alpha, \hat\beta$, just normalize them w.r.t. possible states for $z_n$!
\begin{equation}
\begin{split}
\alpha(z_n) &= P(x_n | z_n) \sum_{z_{n-1}}^{} \hat\alpha(z_{n-1}) P(z_n | z_{n-1}) \\
\beta(z_n) &= \sum_{z_{n+1}}^{} \hat\beta(z_{n+1}) P(x_{n+1} | z_{n+1}) P(z_{n-1} | z_n) \\
\end{split}
\end{equation}

\paragraph{Missing observations: } Simply marginalize over all possible observations. This corresponds to setting terms $P(x_i | z_i) = 1$.



\section{Viterbi Algorithm}

This algorithm efficiently computes the most likely sequence that resulted in the observations. 

\begin{equation}
\max_{\pmb x} P(\pmb x) = \frac{1}{Z} \max_{x_1}\dots\max_{x_N} \phi_{1,2}(x_1,x_2) \phi_{2,3}(x_2, x_3) \dots \phi_{N-1,N}(x_{N-1}, x_N)
\end{equation}

We really just want to apply the ``pushing in'' principal to the $\max$ operators. We can also incorporate the same message passing scheme to further improve speed. 

\paragraph{Key Formulae: } 
\begin{equation}
\begin{split}
\mu_{x\to f}(x) &= \sum_{\ell \in ne(x)}^{} \mu_{f_\ell \to x}(x) \\
\mu_{f_\ell \to x} &= \max_{x_1, \dots, x_m} [\log(\phi_\ell(x_1, \dots, x_m)) + \sum_{m\in ne(f)}^{} \mu_{x_m \to f}(x_m) ]
\end{split}
\end{equation}


It's also generally a good idea to run this algorithm in the log domain (hence the $\log$s above).

\paragraph{Initialization} 

We initialize the Viterbi algorithm from the leaves of the tree. 
\begin{enumerate}
\item If the leaf is a \textbf{factor node}: $\mu_{f\to x}(x) = \log f(x)$.
\item If the leaf is a \textbf{varaible node}: $\mu_{x\to f}(x) = \log(1) = 0$
\end{enumerate}

The ``upward'' messages to the root node are the easiest. The backward messages are more difficult.

\paragraph{The Algorithm} 

We let $w_{n+1}(z_{n+1}) = \mu_{f_{n+1} \to z_{n+1}}(z_n)$

\begin{equation}
w_{n+1}(z_{n+1}) = \max_{z_n} [\ln( P(z_{n+1} | z_n)) + w_n(z_n) ] + \ln(P(x_{n+1} | z_{n+1}))
\end{equation}

We run this recursively starting at factor node $h$. This is our ``forward pass''. For each node we traverse, we store a ``dictionary'' relating each $z_{n+1}\to \arg \max_{z_n} \ln(P(z_{n+1}) | z_n) + w_n(z_n)$. Then, we trace back starting at $z_N$ using these dictionaries.

\paragraph{Interpretation: } $w_n(z_n)= \max_{z_{1:n-1}} \ln(P(x_{1:n}, z_{1:n}))$. 





\section{Expectation Maximization for HMM}

\paragraph{Goal: } Derive the HMM's parameters $P(z_n | z_{n-1})$, $P(x_n | z_n)$ given $\pmb x$ and either $z_n$ or $\pmb z$.


The big challenge here is the presence of latent variables. We can't just directly use ML/MLE/MMSE parameter estimation -- we have to use a more complicated algorithm (hence EM).


The general procedure will be to estimate $z_n$ and update our transition matrix $A$, prior $\pi= p(z_1)$, and emission $\phi_k = P(x | z)$.

\paragraph{The Algorithm: } 

\begin{enumerate}
\item \textbf{Expectation step:} We start with $\theta^{old} = \{\pi_k, A, \phi_k\}$. 
\begin{itemize}
\item Try to enfer $\pmb z$ given $\pmb x$ using the \textbf{forward backward} algorithm.
\item Let $p(\pmb z | \pmb x, \theta^{old}) = \gamma(z_n)$. 
\item Use ML/MLE to estimate $P(z_{n-1}, z_n | \pmb x, \theta^{old}) = \xi(z_{n-1}, z_n)$.
\end{itemize}

\item \textbf{Maximization Step:} Use $\gamma, \xi$ to update $\theta^{old}\to \theta^{new}$.
\begin{equation}
\begin{split}
\pi_k^{(new)} &= \frac{\gamma(z_1 = k)}{\sum_{j=1}^{k} \gamma(z_1 = k)} \\
A_{i,j}^{(new)} &= \frac{\xi(z_{n-1,j}, z_{n,k})}{\sum_{\ell=1}^{K} \sum_{n=2}^{N} \xi(z_{n-1,j}, z_{n,\ell})} \\
\phi_k^{(new)} &= P^{(NEW)} (x_n, z_n)
\end{split}
\end{equation}

Note that the $\phi_k^{(new)}$ depends on the emission model. See Gaussian mixture model for one method of doing this.


\end{enumerate}










































































\end{document}
