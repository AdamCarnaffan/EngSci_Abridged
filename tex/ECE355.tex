\documentclass[a4paper,12pt]{report}
\usepackage{color}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{listings}
\usepackage{media9}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{mathtools} 
\usepackage{amsmath}
\usepackage{extarrows} 

% \def\reals{{\rm I\!R}}
\def\reals{\mathbb{R}}
\def\integers{\mathbb{Z}}
\def\fft{\xlongleftrightarrow{\mathcal{F}}}
\def\fs{\xlongleftrightarrow{\mathcal{FS}}}


\newtheorem{theorem}{Theorem}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\begin{document}

\title{ECE355: Signal Analysis and Communications}
\author{Aman Bhargava}
\date{September-December 2020}
\maketitle

\tableofcontents

\section{Introduction and Course Information}

This document offers an overview of the ECE355 course. They comprise my condensed course notes for the course. No promises are made relating to the correctness or completeness of the course notes. These notes are meant to highlight difficult concepts and explain them simply, not to comprehensively review the entire course.

Primary course topics include:

\begin{enumerate}
\item Signals and Systems (Chapter 2).
\item Frequency Domain Analysis (Chapters 3-5).
\item Sampling (Chapter 9).
\item Introduction to Communication Systems (Chapter 8).
\end{enumerate}

\paragraph{Course Information}
\begin{itemize}
\item Professor: Ben Liang
\item Course: Engineering Science, Machine Intelligence Option
\item Term: 2020 Fall
\end{itemize}




\chapter{Signal Basics}

\section{Definitions}

\paragraph{Two types of signals: } Continuous ($f(x)$ defined $\forall x \in \reals$) and Discrete $f(n)$ defined $\forall n \in \integers$.

\paragraph{Power and Energy of a signal: }
\begin{itemize}
	\item Power of $x(t)$ is $|x(t)|^2$.
	\item Energy of $x(t)$ is defined on interval $[t_1, t_2]$ as 
	$$E_{[t_1, t_2]} = \int_{t_1}^{t_2} |x(t)|^2 dt$$
	$$E_{n_1 \leq n \leq n_2} = \sum_{n=n_1}^{n_2} |x[n]|^2$$

	\item Average power of in $[t_1, t_2]$:
	$$P_{[t_1, t_2]} = \frac{E_{[t_1, t_2]}}{t_2 - t_1}$$
	$$P_{[n_1, n_2]} = \frac{E_{n_1 \leq n \leq n_2}}{n_2 - n_1 + 1}$$
	\item Total Energy: 
	$$E_{\infty} = \lim_{T\to\infty} \int_{-T}^T |x(t)|^2 dt$$
	$$E_{\infty} = \lim_{N\to\infty} \sum_{n = -N}^{N} |x[n]|^2$$
\end{itemize}


\section{Signal Transformations}

\paragraph{Time Shifting: } \textit{Shifts $t_0$ units RIGHT}
$$y(t) = x(t-t_0)$$
$$y[n] = x[n-n_0]$$

\paragraph{Time Scaling: } \textit{Speeds original signal up by factor $a$)} (or slowed down by factor $\frac{1}{a}$). Time reversal occurs when $a<0$.
$$y(t) = x(at)$$

\paragraph{Continuous Scaling AND Shifting: } It is important to remember the following steps for $y(t) = x(at+b)$

\begin{enumerate}
\item \textbf{SHIFT}: $v(t) = x(t+b)$.
\item \textbf{SCALE}: $y(t) = v(at)$.
\end{enumerate}

\paragraph{Discrete Time Scaling AND Shifting: } Remember to IGNORE fractional indexes. Interpolation for `slowing down' a signal is a poorly defined process that will be covered later.

\section{Periodic Signals}

\paragraph{Definition: } A signal is periodic iff $\exists T > 0$ s.t. $x(t+T) = x(t)$ $\forall t\in\reals$. 

\begin{itemize}
\item $T$ is the period of the signal.
\item \textbf{Fundamental} period is the smallest possible $T$.
\item If $x(t)$ is constant, then the fundamental period is undefined.
\end{itemize}

\section{Even and Odd Signals}

\paragraph{Even: } $x(t) = x(-t)$
\paragraph{Odd: } $x(-t) = -x(t)$

\textbf{ANY SIGNAL} can be decomposed into an even and odd component.

$$x_{even}(t) = \frac{1}{2} (x(t) + x(-t))$$
$$x_{odd}(t) = \frac{1}{2} (x(t) - x(-t))$$

$$x(t) = x_{even}(t) + x_{odd}(t)$$


\section{Complex Exponential}
\paragraph{Function Family: } $x(t) = c e^{at}$, $c, a\in \mathbb{C}$

\subsection{Complex Number Review}
\begin{itemize}
\item $z = a+jb$, $z\in\mathbb{C}$, $a, b \in \mathbb{R}$, $j = \sqrt{-1}$.
\item Magnitude = $r=|z| = \sqrt{a^2 + b^2}$.
\item Angle (phase) = $\theta = \arctan(\frac{b}{a})$.
\item $z = re^{j\theta} = r(\cos\theta + j\sin\theta)$.
\end{itemize}

\subsection{Useful Sinusoid Shortcuts}
$$\cos\theta = \frac{e^{j\theta} + e^{-j\theta}}{2}$$
$$\sin\theta = \frac{e^{j\theta} - e^{-j\theta}}{j2}$$

\subsection{Periodic Case}
Letting $c = 1$:
$$x(t) = e^{j\omega_0 t} = cos(\omega_0 t) + j\sin(\omega_0 t)$$
\begin{itemize}
\item Combination of two \textbf{real} signals.
\item $|x(t)| = 1$ $\forall t\in\reals$.
\item \textbf{PERIOD: } $T=\frac{2\pi}{|\omega_0|}$.
\end{itemize}


For more general $c\in\mathbb{C}$: \textit{We let $c = |c|e^{j\phi}$} where $\phi$ is the phase. Then $x(t) = ce^{j\omega_0 t}$, then: 

$$x(t) = |c|e^{j(\omega_0 t + \phi)}$$

For fully general $c, a \in \mathbb{C}$: $x(t) = ce^{at} = ce^{(r+j\omega_0)t}$ where $a = (r+j\omega_0)$

$$x(t) = |c|e^{rt} e^{j(\omega_0 t + \phi)}$$
$$Re\{x(t)\} = |c|e^{rt} \cos(\omega_0 t + \phi)$$

Which leads to two cases (`forced harmonic' when $r>0$, `damped harmonic' when $r < 0$).

\subsection{Discrete Time Complex Exponential} 

$$x[n] = e^{j\omega_0 n} = \cos(\omega_0 n) + j\sin(\omega_0 n)$$

\begin{itemize}
\item Signal `hops' around the unit circle in \textbf{increments of} $\omega_0$.
\item \textbf{NOT ALWAYS PERIODIC!} $\omega_0 \in a2\pi, a\in \mathbb{Q}$ for periodicity to hold.
\item $c\in\mathbb{C}$ just changes magnitude and phase.
\end{itemize}

\section{Unit Step and Impulse}

\subsection{Discrete Time}

\begin{equation}
	u[n] = 
	\begin{cases}
		0 & n < 0 \\
		1 & n \geq 0 \\
	\end{cases}
\end{equation}

\begin{equation}
	\delta[n] = 
	\begin{cases}
		0 & n \neq 0 \\
		1 & n = 0 \\
	\end{cases}
\end{equation}


\paragraph{Important Properties: } 
\begin{itemize}
\item $\delta[n] = u[n] - u[n-1]$
\item $u[n] = \sum_{k=0}^\infty \delta[n-k]$
\item $u[n] = \sum_{m=n}^{-\infty} \delta[m]$, if we let $m = n-k$
\item $u[n] = \sum_{m=-\infty}^n \delta[m]$, if we let $m = n-k$
\end{itemize}

\textbf{Sampling property: } $x[n] \delta[n-n_0] = x[n_0] \delta[n-n_0]$

\subsection{Continuous Time Case}

\begin{equation}
u(t) = 
\begin{cases}
	0 & t < 0 \\
	1 & t > 0 \\
\end{cases}
\end{equation}

\begin{equation}
\delta(t) = \frac{d}{dt} u(t)
\end{equation}

There are some more formal definitions, but this will do for now. Consider it a finite amount of energy in an infinitely small period. 

\paragraph{Important Properties: } 

\begin{itemize}
\item $\int_{\infty} \delta(t) dt = 1 = u(\infty) - u(-\infty)$
\item $u(t) = \int_{\infty}^{t} \delta(\tau) d\tau$
\item $u(t) = \int_{0}^{\infty} \delta(\sigma - t) d\sigma$
\item Sampling still holds: $x(t_0) = \int_{\infty} x(t) \sigma(t-t_0) dt$
\end{itemize}

\section{Basic System Properties}

\begin{enumerate}
\item \textbf{Memoryless} if $y(t_0)$ depends ONLY on $x(t_0)$ $\forall t_0$.
\item \textbf{Invertable} when you can recover input SIGNAL given output SIGNAL (not value).
\item \textbf{Causal}:
\begin{itemize}
\item \textit{Discrete Time: } If $y[n_0]$ does not depend on \textit{future information}.
\item \textit{Continuous Time: } If $y(t_0)$ does not depend on $x(t)$ for $t \geq t_0$.
\item \textbf{Equivalently: } If two inputs are identical for $t < t_0$, the outputs are identical for $t < t_0$.
\end{itemize}
\item \textbf{Stability: } Small input does not lead to infinite output. \textit{Definition: } System is \textbf{bounded-input-bounded-output} (BIBO) if bounded input leads to bounded output.
\item \textbf{Time Invariance: } Shifted input $\to$ shifted output with same time shift. If $y(t) = sin(x(t))$, then for input $x(t-t_0)$, the output is: 
\begin{equation}
\begin{split}
y(t) &= sin(x(t-t_0)) \\
&= y(t-t_0)
\end{split}
\end{equation}
\item \textbf{Linearity: } Must satisfy \textit{additivity} and \textit{homogineity} (in other words: \textbf{superposition}).
$$ax_1(t) + bx_2(t) \to_S ay_1(t) + by_t(t)$$
Where $x_1 \to y_1$, $x_2 \to y_2$.
\begin{itemize}
\item Linear systems commute with scaling and addition. 
\item Scaling then pushing through system is the same as pushing through system and scaling. 
\item Adding signals together then pushing through the system is the same as pushing each individual signal through the system and then adding the outputs together.
\end{itemize}
\end{enumerate}


\paragraph{Initial Rest Condition: } if $x(t) = 0$ $\forall t < t_0$, then the corresponding output $y(t) = 0$ $\forall t< t_0$.


\chapter{Linear Time-Invariant Systems}

\section{Discrete Time LTI Properties}

The system's response to an impulse function $\delta[n]$ is called $h[n]$. \textbf{IF WE KNOW $h[n]$}, we can map any input to its respective output due to \textit{time invariance and linearity}!

\begin{equation}
y[n] = \sum_{k=-\infty}^{\infty} x[k] h[n-k]
\end{equation}

\subsection{Convolution in Discrete Time}

\paragraph{Convolution of $x[n], h[n] \to y[n]$ } is defined as: 
\begin{equation}
y[n] = \sum_{k=-\infty}^{\infty} x[k] h[n-k] \equiv x[n] * h[n]
\end{equation}


Interpretations:
\begin{enumerate}
\item Weighted superposition of time shifted impulse responses (pretty clear).
\item Sum over dimension $k$ of function $x[k]h[n-k]$.
\begin{itemize}
\item $h[n-k] = h[-k+n]$ is \textbf{flipped} and shifted \textbf{right} by $n$.
\item Multiply $h[n-k]$ by $x[k]$ and \textbf{sum} the result to get $y[n]$.
\end{itemize}
\end{enumerate}


\section{Continuous Time LTI Systems}

\paragraph{Unit Impulse Response: } $h(t)$ results from input $\delta(t)$. 

\begin{theorem}
\begin{equation}
\begin{split}
y(t) = \int_{-\infty}^{\infty} x(\tau)h(t-\tau) d\tau \\
y(t) \equiv x(t) * h(t)
\end{split}
\end{equation}
\end{theorem}



\section{Properties of LTI Systems}

\paragraph{Properties of Convolution: } 
\begin{enumerate}
\item \textbf{Convolution is Commutative: } $x(t)*h(t) = h(t)*x(t)$
\item \textbf{Convolution is Associative: } $x(t)*[h_1(t)*h_2(t)] = [x(t)*h_1(t)]*h_2(t)$
\item \textbf{Convolution is Distributive: } $x(t)*[h_1(t) + h_2(t)] = x(t)*h_1(t) + x(t)*h_2(t)$
\end{enumerate}

\paragraph{Identity System: } If the unit impulse response is $\delta(t)$, then the system is the \textbf{identity system} -- it will produce the same output as the input.

\paragraph{Time shift note: } Shifting the input AND the step response yields double that shift. One must only shift one to shift the output correspondingly (time invariance property).

\paragraph{Properties of LTI: } 
\begin{enumerate}
\item \textbf{Memory: } LTI is \textit{memoryless} iff $h(t) = K\delta(t)$ ($K\in\reals$), leading to $y(t) = Kx(t)$ being the \textbf{only memoryless LTI} family.
\item \textbf{Invertibility: } If an LTI is invertible, \textit{its inverse is also an LTI}. 
\item \textbf{Causality: } LTI is causal iff $h(t) = 0\,\,\,\forall t < 0$. \textit{Note that `causality' is interchangable for `initial rest condition'}.
\item \textbf{Stability: } Conditions for LTI stability are as follows (tl;dr: bounded unit impulse response is the necessary and sufficient condition). 
\begin{itemize}
\item \textit{Absolutely Integrable: } $\int_{-\infty}^\infty |h(\tau)|d\tau < \infty$
\item \textit{Absolutely Summable: } $\sum_{n=-\infty}^\infty |h[n]| < \infty$
\end{itemize}
\end{enumerate}


\section{Linear Constant-Coefficient Differential (Difference) Equations: LCCDE's}

\paragraph{General Form for Continuous Time: } 

\begin{equation}
\sum_{k=0}^N a_k \frac{d^k}{dt^k} y(t) = \sum_{k=0}^{M} b_k \frac{d^k}{dt^k} x(t) 
\end{equation}

\begin{itemize}
\item Always assume \textit{initial rest} condition.
\item LCCDE's are a subset of \textit{causal LTI systems}.
\item LCCDE's provide a \textit{close approximation} of most LTI systems. \textit{This is because the ``transfer function'' is a rational function that can easily approximate most functions}.
\end{itemize}

\paragraph{Standard Solution: } We would normally use \textbf{method of undetermined coefficients} -- this is a relatively unsophisticated method, but it's important to keep in mind. 

\begin{enumerate}
\item We are given some $x(t)$ and are asked to solve for $y(t)$ given an LCCDE relationship (e.g. $A\frac{d}{dt}y(t) + B y(t) = x(t)$).
\item Assume a solution $y(t) = y_h(t) + y_p(t)$ where 
\begin{itemize} 
\item $y_h$ is the solution for the case of $x(t) = 0\,\,\forall t$. This is the \textit{natural response} or the \textit{unforced response}.
\item $y_p$ is the solution for the given $x(t)$.
\end{itemize}

\item We guess $y_h(t)$ is of the form $Ae^{st}$. We can substitute into the homogenous equation to solve a relationship between $A, s$ and initial conditions.
\item For $y_p(t)$ we guess again (usually the same form as $x(t)$). We substitute in and solve for coefficients. 
\item Finally, we solve for remaining unknown coefficients given some initial conditions.
\end{enumerate}


\paragraph{Better tools for solving LTI/LCCDES's: } Solve them in the frequency domain using the Fourier transform and Laplace transform! 

\paragraph{How to solve LTI via Transfer Function Laplace Transform} 
\textbf{Example: } $\frac{d}{dt} y(t) + 4y(t) = x(t)$. 

\begin{enumerate}
\item Assume there exists some $H(s)$ (eigenvalue of the eigenfunction family $e^{st}$). 
\item Therefore, $H(s) [\frac{d}{dt} e^{st} = 4e^{st}] = e^{st}$
\item We can now solve for $H(s) = \frac{1}{s+4}$
\item If we can write the input $x(t)$ as the sum of complex exponentials, we can solve for $y(t)$!
\item For the case $x(t) = \cos(\pi t) = 0.5 e^{j\pi t} + 0.5 e^{-j\pi t}$, the corresponding output would be: 
\begin{equation}
y(t) = \frac{1}{2} \frac{1}{(j\pi)+4} e^{(j\pi)t} + \frac{1}{2} \frac{1}{-j\pi+4} e^{-j\pi t}
\end{equation}
\end{enumerate}




\section{Discrete Case for LCCDE's}

\textit{Stands for Linear Constant Coefficient Difference Equations}: 

\paragraph{General form for discrete LCCDE: } 

\begin{equation}
\hat{a}_0 y[n] + \sum_{k=1}^N \hat{a}_n (y[n] - y[n-k]) = \hat{b}_0 x[n] + \sum_{k=1}^M \hat{b}_n(x[n] - x[n-k])
\sum_{k=0}^N a_k y[n-k] = \sum_{k=0}^M b_k x[n-k]
\end{equation}

\paragraph{Solution Options: } 
\begin{enumerate}
\item We could solve in a manner similar to the CT case: 
\begin{equation}
y[n] = y_h[n] + y_p[n]
\end{equation}

\item That said, there is a \textbf{more efficient} method that takes advantage of the discrete nature of this problem. By rearranging, we get: 
\begin{equation}
y[n] = \frac{1}{a_0} [\sum_{k=0}^M b_k x[n-k] - \sum_{k=1}^N a_k y[n-k]]
\end{equation}
\end{enumerate}



\chapter{Fourier Representations of Periodic Signals}

\section{LTI Response to Complex Exponentials}

It turns out that the guess of $y_p(t) = Ae^{st}$ is \textbf{always} a good guess for LTI systems. In fact, it is \textbf{SCALED} every time! Given a system with an impulse reponse $h(t)$ that is fed an input of $e^{st}$: 

\begin{equation}
\begin{split}
y(t) &= \int_{-\infty}^{\infty} h(\tau) e^{s(t-\tau)} d\tau \\
&= e^{st} \int_{-\infty}^\infty h(\tau) e^{-s\tau} d\tau \\
&= e^{st} H(s)
\end{split}
\end{equation}

Where $H(s) \equiv \int_{-\infty}^\infty h(\tau) e^{-s\tau} d\tau$ is the \textbf{Laplace Transform} of $h$ (a.k.a. the \textbf{transfer function}).

\begin{theorem}
If $x(t) = e^{st}$ and $H(s)$ exists: 
\begin{equation}
y(t) = H(s)e^{st}
\end{equation}
In essence, the response of the LTI is a scaled version of the same complex exponential by factor $H(s)$ defined above.
\end{theorem}

\subsection{Discrete Time Case}

Let $e^{sn} \equiv z^n$. $z^n \to h[n] \to y[n]$. We define

\begin{equation}
H(z) = \sum_{k=-\infty}^{\infty} h[k] z^{-k}
\end{equation}
As the \textbf{$z$-transform}. 

\begin{theorem}
$y[n] = H[z]z^n$. 
\begin{itemize}
\item $z^n$ is therefore an \textbf{eigenfunction} of any LTI.
\item $H[z]$ is the corresponding \textbf{eigenvalue} of that eigenfunction.
\end{itemize}
\end{theorem}


\paragraph{Cautionary Notes: } 
\begin{itemize}
\item $(e^x)^z$ does not hold in general for $x, y, \in \mathbb{C}$.
\item $(e^x)^n$ DOES hold for $n\in \mathbb{Z}$.
\end{itemize}

\section{Continuous Time Fourier Series}

\paragraph{Guiding Fact: } Almost all periodic signals are approximated by a sum of weighted \textbf{harmonically related} complex exponentials.

\begin{equation}
x(t) = \sum_{k=-\infty}^{\infty} a_k e^{jk\omega_0 t}
\end{equation}

\paragraph{Note on utilizing frequency response with Fourier series signal representation: } If $x(t) = \sum_{k}^{} a_k e^{jk\omega_0 t}$: 

\begin{equation}
y(t) = \sum_{k=-\infty}^{\infty} a_k H(jk\omega_0) e^{jk\omega_0 t}
\end{equation}

\paragraph{Finding Laplace Transform for LTI: } If given the system in implicit form: \textbf{input the eigenfunction} $x(t) = e^{st}$

\paragraph{Result of Passing Signal through LTI: } 
\begin{enumerate}
\item Frequency-dependent \textbf{amplification} 
\item Frequency-dependent \textbf{phase shift} 
\end{enumerate}

\subsection{Calculating CT Fourier Series}

\begin{equation}
a_k = \frac{1}{T} \int_{T} x(t) e^{-jk\omega_0 t} dt
\end{equation}

Where $\int_{T}$ is integration over any period of length $T$.



\subsection{Convergence of Fourier Series}

\paragraph{Which periodic signals have Fourier series representations? } 

\begin{theorem}
We define the finite fourier series 
\begin{equation}
x_n(t) = \sum_{k=-N}^{N} a_k e^{j\omega_0 kt}
\end{equation}
where the error is $e_N(t) = x(t) - x_N(t)$, $E_N \int_{T}^{} |e_n(t)|^2 dt$. 

\textbf{If $x(t)$} has finite energy in one period, then 
\begin{equation}
\lim_{N\to \infty} E_N = 0
\end{equation}

\end{theorem}

If $x(t)$ satisfies \textbf{Dirchlet conditions} (nearly all signals), then $x(t) = FS(x(t))$ except at \textbf{isolated points}.

\paragraph{Gibbs Phenomena: } Small oscillations about discontinuities in a signal (e.g. approximations of a square wave).

\section{Properties of Continuous Time Fourier Series}

\paragraph{Setup: } Let $x(t)$ be periodic with fundamental period $T\to \omega_0 = \frac{2\pi}{T}$ that has Fourier series coefficients $a_k$.

\paragraph{Properties: } 

\begin{enumerate}
\item \textbf{Linearity: } 
\begin{equation}
Ax(t) + By(t) \underrightarrow{\,\,\mathcal{F}\,\,} Aa_k + Bb_k
\end{equation}

\item \textbf{Time Shift: } 
\begin{equation}
x(t-t_0) \underrightarrow{\,\,\mathcal{F}\,\,} e^{-jk\omega_0 t_0} a_k
\end{equation}


\item \textbf{Time Scaling: } For $\alpha > 0$: 
\begin{equation}
x(\alpha t) \underrightarrow{\,\,\mathcal{F}\,\,} a_k
\end{equation}

Where $a_k$ now has fundamental period $\alpha \omega_0$

\item \textbf{Time Reversal: } 
\begin{equation}
x(t) \underrightarrow{\,\,\mathcal{F}\,\,} a_{-k}
\end{equation}
Therefore even functions have $a_k = a_{-k}$ and odd functions have $-a_k = a_{-k}$.

\item \textbf{Conjugation: } 
\begin{equation}
x^* (t) \underrightarrow{\,\,\mathcal{F}\,\,} a_{-k}^*
\end{equation}
\begin{itemize}
\item Special case: $x(t)$ is a real signal so $x^*(t) = x(t)$. The we have $a_k = a_{-k}^*$
\item Known as ``conjugate symmetric'' or ``Hermitian''.
\item If you know $x(t)$ is real, then: $$x(t) = a_0 \sum_{k=1}^{\infty} 2 A_k \cos (k\omega_0 t + \theta)$$
\textit{Where} $a_k = A_k e^{j\theta_k}$.
\end{itemize}

\item \textbf{multiplication: } Multiplication in one domain corresponds to convolution in the other!
\begin{equation}
x(t) y(t) \underrightarrow{\,\,\mathcal{F}\,\,} c_k = \sum_{l = -\infty}^{\infty} a_l b_{k-l}
\end{equation}

\item \textbf{Parsevel's Transform: } The average power of $x(t)$ is given by: 

\begin{equation}
\frac{1}{T} \int_{T}^{} |x(t)|^2 dt = \sum_{k=-\infty}^{\infty} |a_k|^2
\end{equation}

\end{enumerate}
\textit{Other properties can be found in table 3.1}

\section{Continuous Time Fourier Transform}

\textit{Recap: } Fourier series can approximate nearly all periodic signals. we now introduce the Fourier Transform, a system to approximate aperiodic signals!

\begin{theorem}

We define the Fourier Transform as follows: 

\begin{equation}
X(j\omega) = \int_{-\infty}^{\infty} x(t) e^{-j\omega t} dt
\end{equation}

And its inverse operation as: 

\begin{equation}
x(t) = \frac{1}{2\pi} \int_{-\infty}^{\infty} X(j\omega) e^{j\omega t} d\omega
\end{equation}
\end{theorem}

\paragraph{Important Notes on the Fourier Transform Properties: } 

\begin{itemize}
\item $\text{sinc}(x) \equiv \frac{\sin(\pi x)}{\pi x}$
\item Wider signal in time domain leads to narrower signal in frequency domain.
\item $x(t) = \delta(t) \to X(j\omega) = 1$
\item $X(j\omega) = 2\pi \delta(\omega) \to x(t) = 1$
\item $x(t) = u(t) \to X(j\omega) = \delta(\omega) \pi + \frac{1}{j\omega}$
\end{itemize}

\subsection{Periodic Signal Fourier Transform}

\paragraph{Key struggle: } Periodic signals have infinite energy and therefore do not converge in the Fourier transform integral. 

\begin{theorem}
For an arbitrary periodic $x(t) = \sum_{k = -\infty}^{\infty} a_k e^{jk\omega_0 t}$:
\begin{equation}
X(j\omega) = \sum_{k = -\infty}^{\infty} 2\pi a_k \delta(\omega - k\omega_0)
\end{equation}

In other words, a periodic signal is simply a collection of delta functions in the frequency domain. 
\end{theorem}

Steps to find the Fourier Transform of periodic signal: 
\begin{enumerate}
\item Find Fourier Series version of $x(t)$.
\item Use $a_k$ in the above formula $X(j\omega) = \sum_{k=-\infty}^{\infty} 2\pi a_k \delta(\omega - k\omega_0)$.
\end{enumerate}



\subsection{Properties of Continuous Time Fourier Transform}

\begin{enumerate}
\item \textbf{Linearity: } \begin{equation}
ax(t) + by(t) \fft aX(j\omega) + bY(j\omega)
\end{equation}

\item \textbf{Time Shift: } \begin{equation}
x(t-t_0) \fft e^{-j\omega t_0} X(j\omega)
\end{equation}

\item \textbf{Time + Frequency Scaling: } \begin{equation}
x(at) \fft \frac{1}{|a|} X(\frac{j\omega}{a})
\end{equation}

\item \textbf{Conjugation: } \begin{equation}
x^*(t) \fft X^*(-j\omega)
\end{equation}

\item \textbf{Differentiation and Integration: } 
\begin{align}
x'(t) \fft j\omega X(j\omega) \\
\int_{-\infty}^{t} x(\tau) d\tau \fft \frac{1}{j\omega} X(j\omega)
\end{align}

\item \textbf{Duality: } If $x(t) \fft X(j\omega) = g(\omega)$ then $g(t) \fft 2\pi x(-\omega)$.

\item \textbf{Frequency Shifting: } \begin{equation}
e^{j\omega_0 t} x(t) \fft X(j(\omega-\omega_0))
\end{equation}


\item \textbf{Differentiation in Frequency Domain: } \begin{equation}
-jtx(t) \fft \frac{d}{d\omega} X(j\omega)
\end{equation}



\item \textbf{Integration in Frequency Domain: } \begin{equation}
\frac{-1}{j\omega} x(t) + \pi x(0) \delta(t) \fft \int_{-\infty}^{\omega} X(j\eta) d\eta
\end{equation}

\item \textbf{Parseval's Relation: } \begin{equation}
\int_{-\infty}^{\infty} |x(t)|^2 dt = \frac{1}{2} \int_{-\infty}^{\infty} |X(j\omega)|^2 dw
\end{equation}

\end{enumerate}

\subsection{Special Properties: Multiplication and Convolution}

Convolution and multiplication are the two most important properties to deeply understand about Fourier Transform. They let you use $\mathcal{F}(h(t)) = H(s)$ as you might use the laplace transform of $H$, but this time, you get more information about the \textbf{steady-state} harmonic response. 

\paragraph{Convolution: } \begin{equation}
x(t)*h(t) \fft X(jw)H(jw)
\end{equation}
\begin{itemize}
\item \textbf{Eigen function:} $e^{st} \to \text{any LTI with }h(t) \to H(s)e^{st}$
\begin{itemize}
\item Since we can break down any signal into the weighted sums of $e^{st}$ terms with $s=j\omega$, we can determine frequency response for any function by multiplying to $H(s)$ in the frequency domain. 
\end{itemize}
\item \textbf{Key point:} System with impulse reponse $h(t)$ and input $x(t)$ has output $\mathcal{F}^{-1}(H(jw)X(jw)) = \mathcal{F}(Y(jw)) = y(t)$.
\item An idealized low-pass filter is therefore a ``box'' in the frequency domain and a $sinc$ function in the time domain. 
\end{itemize}



\paragraph{Multiplication: } \begin{equation}
x_1(t)x_2(t) \fft \frac{1}{2\pi} (X_1*X_2)(jw) 
\end{equation}

%&= \frac{1}{2\pi} \int_{-\infty}^{\infty} X_1(j\theta) X_2(jw-j\theta)d\theta



\section{Solving LCCDE's with Fourier Transform}

\textit{This was high-key on the midterm before it was taught, so I put it in section 2.4 as well.} 

\paragraph{Recall: } LCCDE's relate some output $y(t)$ to an input $x(t)$ in the following form: 
\begin{equation}
\sum_{k=0}^{N} a_k \frac{d^k}{dt^k} y(t) = \sum_{k=0}^{M} b_k \frac{d^k}{dt^k} x(t)
\end{equation}

\paragraph{Frequency Response for an LCCDE: } 
\begin{equation}
H(s) = \frac{\sum_{k=0}^{M} b_k s^k}{\sum_{k=0}^{N} a_k s^k} 
\end{equation}

Since it's a rational function, you can approximate any $H(s)$ well. To get the \textbf{impulse response} $h(t)$, just compute $\mathcal{F}^{-1}(H(s))$.

You should probably use partial fractions to do that, it makes life a lot easier. 


\section{Discrete Time Fourier Series}

\begin{itemize}
\item $x[n]$ has period $N$ $\to$ $w_0 = \frac{2\pi}{N}$
\item $e^{jk\omega_0 n}$ is \textbf{harmonically related} for $k\in \mathbb{N}$.
\end{itemize}

\begin{theorem}{Discrete Time Fourier Series: }
\paragraph{Synthesis Equation: } 
\begin{equation}
x[n] = \sum_{k\in \langle N \rangle}^{} a_k e^{jk\omega_0 n}
\end{equation}

\paragraph{Analysis Equation: } 
\begin{equation}
a_k = \frac{1}{N} \sum_{n\in \langle N \rangle}^{} x[n] e^{-jk\omega_0 n}
\end{equation}

\begin{itemize}
\item $n\in \langle N \rangle$ just means $n$ in any \textbf{continguous set} of $N$ integers. 
\item \textbf{DTFS always converges!} It's basically projecting the vector of size $N$ (that repeats, but whatever) onto $N$ linearly independent basis vectors $e^{jk\omega_0 n}$. The linear algebra checks out (this is an extention of course content). 
\item $a_k$ is \textbf{periodic} with period $N$ (it therefore makes little difference to represent $k$ for all integers). 
\end{itemize}
\end{theorem}

\section{Discrete Time Fourier Transform}

\textit{Intuition: Like for the continuous time case, we take the limit as $N\to \infty$}.

\begin{theorem}{Discrete Time Fourier Series}
\paragraph{Synthesis Equation: } 
\begin{equation}
x[n] = \frac{1}{2\pi} \int_{2\pi}^{} X(e^{j\omega}) e^{j\omega n} d\omega
\end{equation}

\paragraph{Transform/Analysis Equation: } 
\begin{equation}
X(e^{j\omega} = \sum_{n=-\infty}^{\infty} x[n] e^{-j\omega n}
\end{equation}

\end{theorem}

\paragraph{Notes on DTFT: } 
\begin{itemize}
\item Note that $e^{j\omega}$ is only allowable as the argument for $X$ because $e^{j\omega n} = (e^{j\omega})^n$.
\item $X(e^{j\omega})$ is periodic with $T=2\pi$.
\item Convergence does have some conditions, but they can be assumed for all problems in this class. 
\item Values at $0, 2\pi$ correspond to \textbf{lower frequencies} while values at $\pi$ correspond to the highest possible frequency in discrete time. 
\end{itemize}

\begin{theorem}{DTFT For Periodic Signals}
For any periodic signal $x[n]$ \begin{equation}
x[n] = \sum_{k\in \langle N \rangle}^{} a_k e^{jk\omega_0 n}
\end{equation}
Where $\omega_0 = \frac{2\pi}{N}$, we have DTFT 
\begin{equation}
X(e^{j\omega} = \sum_{k = -\infty}^{\infty} 2\pi a_k \delta(\omega-k\omega_0)
\end{equation}
\end{theorem}


\subsection{Properties of Discrete Time Fourier Transform}

\begin{enumerate}
\item Periodic on $2\pi$: \begin{equation}
X(e^{j(\omega+2\pi)} = X(e^{j\omega})
\end{equation}

\item Linearity: 
\begin{equation}
a_1 x_1[n] + a_2 x_2[n] \fft a_1 X_1(e^{j\omega}) + a_2 X_2(e^{j\omega})
\end{equation}

\item Time and Frequency Shift: 
\begin{align}
x[n-n_0] \fft e^{-j\omega n_0} X(e^{j\omega}) \\ 
e^{j\omega_0 n} x[n] \fft X(e^{j(\omega-\omega_0)})
\end{align}

\item Conjugation: 
\begin{equation}
x^*[n] \fft X^*(e^{-j\omega})
\end{equation}
\begin{itemize}
\item $x[n] \in \mathbb{R} \to X(e^{j\omega}) = X^*(e^{j\omega})$ (``Conjugate Symmetry'').
\item $\text{Even}(x[n]) \fft \Re\{X(e^{j\omega})\}$
\item $\text{Odd}(x[n]) \fft j\Im\{X(e^{j\omega})\}$
\end{itemize}


\item Differencing and Accumulation
\begin{itemize}
\item Difference: 
\begin{equation}
x[n] - x[n-1] \fft (1-e^{-j\omega})X(e^{j\omega})
\end{equation}
\begin{equation}
x[n] - x[n-k] \fft (1-e^{-j\omega k}X(e^{j\omega}))
\end{equation}

\item Accumulation: 
\begin{equation}
\sum_{m=-\infty}^{n} x[m] = x[n] * u[n] \fft X(e^{j\omega})U(e^{j\omega}) 
\end{equation}

\begin{equation}
= \frac{1}{1-e^{-j\omega}} X(e^{j\omega}) + \pi X(e^{(j\theta)}) \sum_{k=-\infty}^{\infty} \delta(\omega - 2\pi k)
\end{equation}
\end{itemize}

\item Time Reversal:
\begin{equation}
x[-n] \fft X(e^{-j\omega} )
\end{equation}

\item Time Scaling: $x[an]$ is not defined for all $a\notin \mathbb{Z}$. 
\begin{itemize}
\item Time Contraction: $|a| > 1, a\in \mathbb{Z}$ has \textbf{no general formula}.
\item Time Expansion: 
\begin{equation}
x_{(k)}[n] \equiv \begin{cases}
x[n/k], & \text{if }n\text{ is a multiple of } k \\
0, & \text{else}
\end{cases}
\end{equation}
\begin{equation}
X_{(k)}(e^{j\omega}) = X(e^{jk\omega})
\end{equation}
\end{itemize}


\item Differentiating in Frequency: 
\begin{equation}
-jn x[n] \fft \frac{d}{d\omega} X(e^{j\omega})
\end{equation}


\item Parseval's Relation: 
\begin{equation}
\sum_{n=-\infty}^{\infty} |x[n]|^2 = \frac{1}{2\pi} |X(e^{j\omega})|^2 d\omega
\end{equation}


\item Convolution in Time: 
\begin{equation}
x[n] * h[n] \fft X(e^{j\omega})H(e^{j\omega})
\end{equation}


\item Multiplication in Time: 
\begin{align}
x_1[n]x_2[n] \fft (X_1 *_p X_2)(e^{j\omega}) \\
= \frac{1}{2\pi} \frac{2\pi}{} X_1(e^{j\theta})X_2(e^{j(\omega - \theta)}) d\theta
\end{align}


\item Duality -- From CTFT to DTFT
\begin{equation}
x[n] \fft X(e^{j\omega}) \fs x[-n]
\end{equation}
\end{enumerate}



\section{LCCDE in Discrete Time}

\paragraph{Form of LCCDE: } 
\begin{equation}
\sum_{k = 0}^{N} a_k y[n-k] = \sum_{k=0}^{M} b_k x[n-k]
\end{equation}

\begin{equation}
H(e^{j\omega}) = \frac{Y(e^{j\omega})}{X(e^{j\omega})} = \frac{\sum_{k=0}^{M} e^{-j\omega k}}{\sum_{k=0}^{N} a_k e^{-j\omega k}}
\end{equation}

Then simply use the resulting rational function $\to$ partial fraction $\to$ inverse DTFT. 


\chapter{Communications Theory}



\section{Reconstruction from Samples}

\paragraph{Recall: Impulse Train Sampling -- } We multiply $x(t)$ with $p(t)$ (the ``picket fence'' function $p(t) = \sum_{n=-\infty}^{\infty} \delta(t-nT)$) to get $x_p(t)$ -- the sampled version of $x(t)$. It looks like a bunch of delta functions that match the ``value'' of $x(t)$ at each instance $t = nT$ where one exits. 

\paragraph{Guiding Question: } How do we \textbf{interpolate} between discrete points recorded for a continuous waveform? 

\paragraph{Answer: } Use an \textbf{ideal low-pass filter}. Make sure you visually understand the following things first.
\begin{itemize}
\item $x_p$ is $X(j\omega)*P(j\omega)$, which corresponds to \textbf{a bunch of copies of $X(e^{j\omega})$} spaced $w_s = \frac{2\pi}{T}$ apart. 
\item To prevent \textbf{overlap in the frequency domain (aliasing)}, the \textbf{band limit} of the signal must be $w_c \leq w_s/2$ (draw it out to visualize it). This is the Nyquist rate.
\item We can recover $X(jw)$ by low-pass filtering $x_p(t)$ under $w_c \leq w_s/2 = \pi/T$. 
\end{itemize}



And honestly, that's the central idea of the communications theory we learn in this course! For the theoretically perfect case, just convolve the sampled $x_p(t)$ with an idealized low-pass filter $sinc$ function: 

\begin{align}
x_r(t) &= x_p(t) * h(t) \\ 
&= \sum_{n=-\infty}^{\infty} x(nT)\delta(t-nT)*h(t) \\
x_r(t) = x(t) &= \sum_{n=-\infty}^{\infty} x(nT)h(t-nT)
\end{align}

Which is pretty wild. Literally it just boils down to multiplying a bunch of $sinc$ functions \textbf{by the value of $x_p(t)$} at each sampled point. The only problem is that it is not \textbf{causal} and is therefore not practical in real time to perform idealized low-pass filtering. 


\paragraph{Signal Reconstruction -- Key Facts and Equations: } In summary,
\begin{itemize}
\item Input signal is band limited by $\omega_m$.
\item Sampling occurs with period $T$ and frequency $\omega_s$ using function $p(t)$, a bunch of delta functions spaced $T$ distance apart in time. 
\item \textbf{Nyquist Rate: } $w_s \geq 2\omega_m$.
\item To reconstruct $x(t)$, we \textbf{low-pass filter} $x_p(t)$ with an ideal low-pass filter with bounds $\omega_c$ which must be between $\omega_m$ and $\omega_s/2$. By default, we use $\omega_c = \frac{\omega_s}{2} = \frac{\pi}{T}$.
\end{itemize}

\subsection{Practical Interpolation Strategies}

Since an ideal low pass filter is, well, \textit{ideal}, we have a few methods to approximate the original signal in real time. 

\paragraph{Zero-Order Hold: } Literally just keep the reconstruction constant as new samples are received.
\begin{equation}
x_r(t) = x(kT) \text{ for }kT \leq t \leq (k+1)T
\end{equation}
\begin{itemize}
\item Frequency response: $x_r(t) = \sum_{n=-\infty}^{\infty} x(nT) h_0(t-nT)$, where $h_0(t)$ is 
\begin{align}
h_0(t) &= \begin{cases}
1 & \text{ if } t\in [0,T]  \\
0 & \text{ otherwise}
\end{cases} \\
H_0(jw) &= e^{-j\omega\frac{T}{2}} (\frac{2\sin(\omega\frac{T}{2})}{\omega}) \\
\end{align}

\item \textbf{Limits on Error: } You get a bit of high-frequency distortion from the left and right lobes of the $X_p$ signal.
\end{itemize}

\paragraph{First-Order Hold: } Simply applying linear interpolation ($x_r(t)$ is weighted average of two closest samples).
\begin{equation}
x_r(t) = \frac{(k+1)T-t}{T} x(kT) + \frac{t-kT}{T} x((k+1)T)\,\,\forall kT\leq t\leq (k+1)T
\end{equation}
\begin{itemize}
\item Frequency response: $x_r(t) = \sum_{n=-\infty}^{\infty} x(nT)h_1(t-nT)$ where 
\begin{align}
h_1(t) &= \begin{cases}
\frac{t}{T} +1 & \text{ for } t\in [-T,0] \\
\frac{-t}{T} +1 & \text{ for } t\in [0, T] \\
0 & \text{elsewhere.}
\end{cases} \\
H_1(jw) &= \frac{1}{T} (\frac{2\sin(\omega\frac{T}{2})}{\omega})^2 
\end{align}
\item Limits on Error: Reduces the error seen in the 0-order hold approximation, as one would expect from increasing... the order... of the approximation.
\end{itemize}

\subsection{Under Sampling and Aliasing}

\paragraph{Under Sampling: } When the sampling rate results in overlapping `lobes' for $X_p(j\omega)$. That is, when $\omega_s < 2\omega_m$.
\begin{itemize}
\item High frequency energy is added from the tails of the left and right `lobes'. 
\item It is visible in the time domain that information is being lost.
\item Due to overlap, you can get \textbf{much lower frequencies} in the reconstructed signals, due to overlap of lobes in time domain. 
\item Same phenomena as the wagon wheel effect.
\end{itemize}

\section{Discrete Time Sampling}






















\end{document}

