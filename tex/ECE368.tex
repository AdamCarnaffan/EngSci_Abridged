\documentclass[a4paper,12pt]{report}
\usepackage{color}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{listings}
\usepackage{media9}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{mathtools} 
\usepackage{amsmath}
\usepackage{extarrows} 

% \def\reals{{\rm I\!R}}
\def\reals{\mathbb{R}}
\def\integers{\mathbb{Z}}
\def\fft{\xlongleftrightarrow{\mathcal{F}}}
\def\fs{\xlongleftrightarrow{\mathcal{FS}}}


\newtheorem{theorem}{Theorem}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\begin{document}

\title{ECE368: Probabilistic Reasoning}
\author{Aman Bhargava}
\date{January-April 2021}
\maketitle

\tableofcontents

\section{Introduction and Course Information}

\paragraph{Course Information}
\begin{itemize}
\item Professors: Prof. Saeideh Parsaei Fard and Prof. Foad Sohrabi
\item Course: Engineering Science, Machine Intelligence Option
\item Term: 2021 Winter
\end{itemize}

\paragraph{Main Course Topics} 
\begin{itemize}
\item Vector, temporal, and spatial models.
\item Classification and regression model training.
\item Bayesian statistics, frequentist statistics.
\end{itemize}




\chapter{Review Topics}

\textit{See ECE286 notes for further reference} 

\section{Review of Probability Functions}

\paragraph{Probability Mass Function: } For \textit{discrete random variables}, $P_X(x)$denotes the probability that random variable $X$ takes on value $x$.

\paragraph{Probability Density Function: } For \textit{continuous random variables}, the probability $\Pr\{X\in [x_1, x_2]\}$ is given by $\int_{x_1}^{x_2} f_X(x) dx$.

Joint PMF's and PDF's are similarly defined. 

\paragraph{Marginal Probability Distributions: } Given joing PMF $P_{X, Y}(x, y)$ or PDF $f_{X,Y}(x, y)$, we can \textbf{marginalize} them as follows:
\begin{equation}
P_X(x) = \sum_{y\in Y}^{} P_{X,Y}(x, y)
\end{equation}
\begin{equation}
f_X(x) = \int_{-\infty}^{\infty} f_{X,Y}(x,y)dy
\end{equation}

\paragraph{Conditional Probability Functions: } 
\begin{equation}
P_{Y|X}(y, x) = \frac{P_{X,Y}(x,y)}{P_X(x)} 
\end{equation}


\paragraph{Prior Probability: } Probability \textbf{before} an additional observation is made (hence \textit{prior}). Example: $P_X(x)$.

\paragraph{Posterior Probability: } Probability \textbf{after} an observation is made (hence \textit{post}erior). Example: $P_{X|Y}(x, y)$.

\paragraph{Bayes Rule: }
\begin{equation}
P(B|A) = P(A|B)\frac{P(B)}{P(A)}
\end{equation}

\section{Expectation, Correlation, and Independence} 

\paragraph{Expectation Value: } $\mathbb E[x] = \sum_{x\in X}^{} P_X(x) = \int_{-\infty}^{\infty} xf_X(x) dx$

\paragraph{Law or Large Numbers: } $\lim_{N\to\infty} \frac{1}{N} \sum_{i=1}^{N} x_i = \mathbb E[X]$

\paragraph{Variance: } 

\begin{equation}
\begin{split}
\text{Var} (X) &= \mathbb E[(X-\mathbb E[x])^2] \\
&= \mathbb E[X^2] - (\mathbb E[X])^2
\end{split}
\end{equation}

\paragraph{Covariance: } 
\begin{equation}
\begin{split}
\text{Cov}(X,Y) &= \mathbb E[(X-\mathbb E[X]) (Y-\mathbb E[Y])] \\
&= \mathbb E_{XY}[XY] - \mathbb E[X]\mathbb E[Y]
\end{split}
\end{equation}

\paragraph{Correlation Coefficient: } 
\begin{equation}
\rho_{XY} = \frac{\text{Cov}(X,Y)}{\sqrt{\text{Var}(X)}\sqrt{\text{Var}(Y)}} 
\end{equation}
\begin{itemize}
\item $\rho_{XY}\in [-1, 1]$
\item $\rho > 0$ indicates positive correlation (line of best fit has positive slope).
\item $\rho < 0$ indicates negative correlation.
\item \textbf{$\mathbb E[XY] = \mathbb E[X] \mathbb E[Y]$ iff $X,Y$ are uncorrelated}.
\end{itemize}

\paragraph{Independence} 
\begin{theorem}{Independence}
Random variables $X,Y$ are independent \textbf{iff} 
\begin{equation}
P_{XY}(x,y) = P_X(x) \cdot P_Y(y)
\end{equation}
This also means that $\rho_{XY} = 0$, $P(X|Y) = P(X)$, etc.
\end{theorem}



\section{Laws of Large Numbers}

\paragraph{Weak Law: } Sample mean converges to the mean.

\paragraph{Strong Law: } If $\{x_i\}$ are \textbf{independent, identically distributed} (i.i.d.) random variables with mean $\mu$, then the \textbf{probability of} the sample mean = $\mu$ is 1 as $n\to \infty$.

\chapter{Parameter Estimation}

\section{Estimation Terminology}

\begin{itemize}
\item $\hat \theta_n$ is an \textbf{estimator} of some unknown parameter $\theta$.
\item \textbf{Estimation Error: } $\hat \theta_n - \theta$
\item \textbf{Bias} of estimator:  $\mathbb E[\hat \theta_n] - \theta$
\begin{itemize}
\item \textbf{Unbiased} estimator:  Bias$=0 = \mathbb E[\hat \theta_n] - \theta$.
\item \textbf{Asymptotically Unbiased: } $\lim_{n\to \infty} \mathbb E[\hat \theta_n] = \theta$ for all $\theta$.
\end{itemize}
\item \textbf{Consistency: } Estimator is consistent if $\lim_{n\to \infty} \hat \theta_n = \theta$.
\end{itemize}

\section{Maximum Likelihood Estimation}

\paragraph{Framing: } Let random variable $\vec X = [X_1, X_2, ..., X_n]$ be defined by either
\begin{enumerate}
\item Joint PMF $P_{\vec X}(\vec x; \theta)$
\item Joint PDF $f_{\vec X}(\vec x; \theta)$
\end{enumerate}

$\vec x$ is a series of measurements. 

\paragraph{Maximum Likelihood Estimation: } The ML estimate of model parameter $\theta$ is
\begin{equation}
\hat \theta_n = \text{argmax}_\theta P_{\vec X}(\vec x; \theta)
\end{equation}

\paragraph{Independent, identically distributed case: } If each $x_i\in\vec x$ are independent and identically distributed, then

\begin{equation}
P_{\vec X}(\vec x; \theta) = \prod_{i=1}^n P_{X}(x_i; \theta)
\end{equation}

Which we can convert to a summation by taking the \textbf{log-likelihood} (recall that logarithm is monotonically increasing, so maximizing log-likelihood is equialent to maximizing likelihood).

\begin{equation}
\hat\theta_n = \text{arg}\max_{\theta} (\sum_{i=1}^{n} \log P_X(x_i; \theta))
\end{equation}


\section{Frequentist vs. Bayesian Statistics}

\paragraph{Frequentist: } In \textbf{classical statistics}, probability is taken to be approximately equal to the \textbf{frequency of events}. Model parameters are assumed to have some deterministic, fixed value (even though they might be unknown).

\paragraph{Bayesian Statistics: } Model parameters are treated as \textbf{random variables} with their own distributions.
\begin{itemize}
\item Generally the more modern approach.
\item We are most interested in the \textbf{joint probability distribution} of model parameters and model arguments (e.g., $f_x(x, \theta)$).
\item \textbf{Main criticism:} probabilities are assigned to unrepeatable events (arguably violates the definition of probability as the limit of event frequency).
\end{itemize}


\section{Maximum a Posteri Estimation (MAP)}

\begin{equation}
\begin{split}
\hat\theta_{map} &= \text{arg}\max_{\theta} f_{\theta | x} (\theta | x)\\
&= \text{arg}\max_\theta f_{X|\theta} (x | \theta) \frac{f_\theta(\theta)}{f_X(x)} 
\end{split}
\end{equation} 


Where $f_\theta(\theta)$ is the \textbf{prior distribution} of model parameter. 
\begin{itemize}
\item If $f_\theta(\theta)$ is uniform, we will still get the same answer as a \textbf{maximum likelihood} estimation.
\end{itemize}


\subsection{Picking a Prior Distribution}

\paragraph{Best Practice: } Pick a distribution of the same form as $f_{X|\theta}(x|\theta)$ (called ``conjugate pair'').

\paragraph{Beta Distribution: } Used for \textbf{binomial distribution}.
\begin{itemize}
\item Binomial distribution: 
\begin{equation}
P_{X = k | \theta} = {n\choose k} \theta^k(1-\theta)^{n-k}
\end{equation}
where 
\begin{itemize}
\item $\theta$: Probability of success on each Bernoulli trial.
\item $n$: Total number of trials.
\item $k$: Total number of successful trials.
\end{itemize}
\item \textbf{Beta Distribution: } 
\begin{equation}
f_\theta(\theta; \alpha, \beta) = \begin{cases}
c\theta^{\alpha-1}(1-\theta)^{\beta-1} & \text{for } \theta\in [0,1] \\
0 & \text{else}
\end{cases}
\end{equation}

Where
\begin{itemize}
\item $\alpha, \beta$ are customizable parameters.
\item $c = [\Gamma(\alpha + \beta)]/[\Gamma(\alpha)\Gamma(\beta)]$
\item $\Gamma(x) \equiv \int_{0}^{\infty} u^{x-1}e^{-u} du$
\item $\Gamma(x+1) = x\Gamma(x)$ for all $x\in \mathbb R$.
\item $\Gamma(n+1) = n!$ for integer $n$.
\item $\therefore c = \frac{(\alpha+\beta-1)!}{(\alpha-1)!(\beta-1)!}$ for integer $\alpha, \beta$.
\item $\mu_f = \mathbb E[f_\theta(\theta)] = \frac{\alpha}{\alpha + \beta}$
\item Maximum likelihood $\text{arg}\max_\theta f_\theta(\theta) = \frac{\alpha-1}{\alpha+\beta-2}$
\end{itemize}
\end{itemize}


\section{Conditional Expectation Estimator}

\paragraph{Key Idea: } Find the \textbf{expected value} for the estimator given your observations.

\begin{equation}
\hat\theta_{conditional expectation} = \mathbb E[\theta | \vec X = \vec x] = \int_{-\infty}^\infty \theta f_{\theta|\vec x}(\theta | \vec x)
\end{equation}


\section{Bayesian Least Mean Square Estimator (LMS)}

\paragraph{Key Idea: } To estimate random variable model parameter $\theta$, we find

\begin{equation}
\hat \theta_{LMS} = \text{arg}\min_{\hat \theta} \mathbb E [(\theta-\hat\theta)^2]
\end{equation}

\begin{itemize}
\item $\hat \theta_{LMS} = \mathbb E[\theta]$ achieves the goal.
\item \textbf{Equivalently: } We can also find 
\begin{equation}
\hat\theta_{LMS} = \text{arg}\min_{\hat \theta} (\mathbb E[\theta-\hat\theta])^2
\end{equation}
\end{itemize}


\chapter{Hypothesis Testing}

\paragraph{Goal: } Given two hypotheses $H_0, H_1$ and observation $\pmb x = (x_0, \dots, x_n)$, we wish to decide which hypothesis is \textbf{better}.

\paragraph{Error Types: } These are generally with respect to $H_0$, or the ``null hypothesis''.
\begin{itemize}
\item \textbf{Type I:} False \textbf{rejection}. We reject hypothesis $H_i$ despite it being the correct one.
\item \textbf{Type II:} False \textbf{acceptance}. We accept hypothesis $H_i$ despite it being false.
\end{itemize}


\section{Likelihood Ratio Test}

\begin{equation}
\mathbb L(\pmb x) = \frac{P_x(\pmb x; H_1)}{P_x(\pmb x; H_0)} \lessgtr z
\end{equation}

If $\mathbb L(\pmb x) > z$, we \textbf{accept $H_1$}. Else, we accept $H_0$. $z = 1$ corresponds to the maximum likelihood decision rule.

\paragraph{Neyman-Pearson Lemma: } We let $\alpha$ represent the probability of \textbf{false rejection} and $\beta$ represent the probability of \textbf{false acceptance} (i.e., type I and type II error probability respectively).

\begin{equation}
P(\mathbb L(\pmb x) > z ; H_0) = \alpha
\end{equation}
\begin{equation}
P(\mathbb L(\pmb x) \leq z ; H_1) = \beta
\end{equation}

There is a direct tradeoff between $\alpha$ and $\beta$ -- they are inversely proportional (i.e., we cannot get better overall confidence ``for free'' with the same data).

There is equivalence between selecting some likelihood cutoff $z$ and some $\gamma$ for $\pmb x \lessgtr \gamma$. 

\section{Bayesian Hypothesis Testing}

The likelihood ratio test is essentially a \textbf{maximum likelihood} method. Bayesian hypothesis testing is equivalent to \textbf{maximum a posteri} methods.

\begin{itemize}
\item \textbf{Goal: } Choose the most probable hypothesis \textit{given the data}.
\item \textbf{Given: } Hypotheses $\pmb \theta = \{\theta_1, \dots, \theta_M\}$, data $\pmb x = (x_1, \dots, x_n)$.
\item \textbf{Method: } Select the optimal hypothesis based on 
\begin{equation}
\begin{split}
\hat \theta &= \arg\max_{\theta\in \pmb \theta} P(\theta | \pmb x) \\
&= \arg\max_{\theta\in \pmb \theta} P(\pmb x | \theta)P(\theta)
\end{split}
\end{equation}
\end{itemize}

By maintaining $\pmb x$ as a free variable in these computations, the rejector regions $\mathcal R$ for $\pmb x$ can be found relatively easily.


\section{Gaussian Vector Distribution}

\paragraph{Scalar Gaussian Normal Distribution: } 
\begin{equation}
\begin{split}
f_x(x) &= \frac{1}{\sqrt{2\pi\sigma^2}} \exp[\frac{-(x-\mu)^2}{2\sigma^2}] \\
&\Leftrightarrow x \sim \mathcal N(\mu, \sigma^2)
\end{split}
\end{equation}


\paragraph{Gaussian Vector} 
\begin{equation}
f_{\vec x}(\vec x; \vec \mu, \Sigma) = \frac{1}{(2\pi)^{D/2}} \frac{1}{|\Sigma|^{\frac{1}{2}}} \exp [\frac{-1}{2} (\vec x - \vec \mu)^T \Sigma^{-1} (\vec x - \vec \mu)]
\end{equation}
Where 
\begin{itemize}
\item $D$ is the dimension of the vectors $x$.
\item $\vec \mu\in \mathbb R^D$ is the mean vector.
\begin{itemize}
\item $\vec \mu = \mathbb E[\vec x]$
\end{itemize}

\item $\Sigma \in \mathbb R^{D\times D}$ is the covariance matrix. It is \textbf{positive semidefinite}.
\begin{itemize}
\item $\Sigma = \mathbb E [(\vec x - \vec \mu)(\vec x - \vec \mu)^T]$
\end{itemize}


\end{itemize}

\subsection{Eigen Analysis of Gaussian Vectors}

\textit{From ECE367} : All PSD matrices $A$ have \textbf{orthogonal} eigenvectors. We can also arbitrarily scale them to be ortho\textbf{normal}. 
\begin{equation}
A = Q\Lambda Q^T
\end{equation}

Where each column of $Q$ is an eigenvector and $\Lambda = \text{diag}(\text{eigen values})$. $QQ^T = I$ and $Q^T = Q^{-1}$ by orthonormalcy. 

\paragraph{Applying to Gaussian Vectors: } 
We note the term $\frac{-1}{2} (\vec x - \vec \mu)^T \Sigma^{-1} (\vec x - \vec \mu)$. Since $\Sigma$ is PSD we can decompose it into 
\begin{equation}
\begin{split}
\Sigma &= Q \Lambda Q^T \\
\Sigma^{-1} &= Q \Lambda^{-1} Q^T \\
\end{split}
\end{equation}

We define a helper variable $\vec y$ as follows:

\begin{equation}
\begin{split}
& \vec y \equiv Q^T(\vec x - \vec \mu) \\
& \Rightarrow \vec y^T = (\vec x - \vec \mu)^T Q \\
& \Rightarrow (\vec x - \vec \mu)^T \Sigma^{-1} (\vec x - \vec \mu) = \vec y^T Q^T \Sigma^{-1} Q \vec y \\
&= \vec y^T \Lambda^{-1} \vec y^T
\end{split}
\end{equation}

The big payoff is that $\vec y$ is a random variable with a \textbf{diagonal} covariance matrix (i.e., independent components). $\vec y$ also has zero mean!

\begin{equation}
\begin{split}
f_y(\vec y) &= \frac{1}{(2\pi)^{D/2} |\Lambda|^{1/2}} \exp (\frac{-1}{2} \vec y^T \Lambda^T \vec y) \\
&= \prod_{i=1}^D \frac{1}{\sqrt{2\pi\lambda_i}} \exp[\frac{-y_i^2}{2\lambda_i}]
\end{split}
\end{equation}


\section{Gaussian Estimation}

\paragraph{Given: } Observations $x_i\in \mathbb R$ where we know that were generated by $x_i = \theta + w_i$ and $w_i \sim \mathcal N(0, \sigma_i^2)$. In other words, each $x_i$ is a ``measure'' of the same mean value $\theta$, but there is some zero-mean noise $w_i$ with variance $\sigma_i^2$. Note that each data point has its own variance. 

\paragraph{Goal: } Estimate $\theta$.

\subsection{Maximum Likelihood}

\begin{equation}
\begin{split}
\hat \theta_{ML} &= \arg\max_{\theta} f_x(\vec x ; \theta) \\
&= \arg\max_\theta \prod_{i= 1}^n \frac{1}{\sqrt{2\pi} \sigma_i} \exp[\frac{-1(x-\theta)^2}{2\sigma_i^2}] \\
\hat \theta_{ML} &= \frac{\sum_{i=1}^{n} \frac{x_i}{\sigma_i^2}}{\sum_{i=1}^{n} \frac{1}{\sigma_i^2}}
\end{split}
\end{equation}

Intuitively, this corresponds to a weighted sum of each $x_i$. Weight is proportional to $\frac{1 }{\sigma_i^2}$, which corresponds to ``certainty'' in the validity of the data point in representing $\theta$.

\subsection{MAP Estimation}

We assume a \textbf{conjugate prior} distribution for $\theta \sim \mathcal N(x_0, \sigma_0^2)$. Conveniently enough, these $x_0, \sigma_0$ are functionally identical to just having another data point!

\begin{equation}
\begin{split}
\hat \theta_{MAP} &= \arg\max_{\theta} f_{\theta}(\theta) f(x | theta) \\
&= \dots \\
\hat \theta_{MAP} &= \frac{\sum_{i=0}^{n} \frac{x_i}{\sigma_i^2}}{\sum_{i=0}^{n} \frac{1}{\sigma_i^2}} 
\end{split}
\end{equation}

\textbf{Note that the sums now start at zero to incorporate the prior distribution's information!} 

% TODO: First part of March 1 content.


\chapter{Statistical Machine Learning}

\section{Naive Bayesian Classifier}

\textit{Technically this was in the first half, but it fits well to motivate LDA and QDA.} 

\paragraph{Goal: } Let $\theta\in \{1,2\}$ be the class of data points $\vec x \in \mathbb R^n$. We wish to find $P(\theta | \vec x)$.

\paragraph{Naive Bayesian Assumption: } Each component of $\vec x$ is \textbf{independent} with respect to class $\theta$. By the probability axioms relating to independence, 
\begin{equation}
P_{\vec x | \theta}(\vec x | \theta) = \prod_{i = 1}^n P_{x_i | \theta}(x_i | \theta)
\end{equation}


\paragraph{Classification Equations: } 
\begin{equation}
P(\theta | \vec x) = \frac{P(\theta) \prod_{i=1}^n P_{x_i | \theta}(x_i | \theta) }{P(\vec x)} 
\end{equation}
Where $P(\vec x) = \sum_{\theta}^{} [P(\theta) \prod_{i=1}^n P_{x_i|\theta} (x_i | \theta)]$

\begin{equation}
P(\theta = 1) \prod_{i=1}^n P_{x_i | \theta}(x_i | \theta = 1) \lessgtr P(\theta = 2) \prod_{i=1}^n P_{x_i | \theta}(x | \theta = 2)
\end{equation}


\paragraph{Bag of words: } For classifying text, we choose a set of $W$ of the most common words. In our vectors $\vec x$, each index $x_i$ corresponds to whether word $w_i$ appears in the given text. 

\begin{equation}
P_{x_i | \theta}(x_i = w_d | \theta = j) = \frac{\text{occurrences of } w_i \text{ in set }j}{\text{total words in set }j} 
\end{equation}


\paragraph{Laplace Smoothing: } Even if we perform the computation in the log domain, we might run into a $P_{x_i|theta}(x_i | \theta) = 0$ if we lack a datapoint (word occurrence) for a given class. Laplace smoothing simply appends the vocabulary set $W$ to each class (i.e., all words in the set occur at least once a priori).




\section{Linear Discriminant Analysis (LDA)}

\paragraph{Goal: } Create algorithm $\text{LDA}: \vec x_i \to c_i$ where $\vec x_i$ is a data point and $c_i\in C$ is the class it belongs to.

\paragraph{General Methodology: } We use \textbf{Bayesian hypothesis testing} with variable classes we model with normal distributions. An important simplifying assumption is that each class $c$ has different $\vec \mu_c$ \textbf{BUT} they all have the same $\Sigma$. This is what makes our decision boundaries \textbf{linear}!

\paragraph{Classification Equations: } 
\begin{enumerate}
\item $\hat y(\vec x) = \arg\max_{c\in C} \beta_c^T \vec x + \gamma_c$ where
\begin{itemize}
\item $\beta_c = \Sigma^{-1} \mu_c$
\item $\pi_c = P(c)$ (prior probability of class $c$)
\item $\gamma_c = \log(\pi_c) - \frac{1}{2} \mu_c^T \Sigma^{-1} \mu_c$
\end{itemize}

\item Posterior probability of class $c$: 
\begin{equation}
P(c | \vec x) = \frac{\exp(\beta_c^T x + \gamma_c)}{\sum_{c'\in C}^{} \exp(\beta_{c'}^T x + \gamma_{c'})} 
\end{equation}
\end{enumerate}


\section{Quadratic Discriminant Analysis (QDA)}

\paragraph{Goal: } Same as LDA. 
\paragraph{Assumptions: } We are given $\mu_c, \Sigma_c, \pi_c$ for each class, and each $\vec x_i \sim \mathcal N(\mu_c, \Sigma_c)$ for some $c\in C$.

\paragraph{Classification Equations: } 
\begin{equation}
\hat y(\vec x) = \arg\max_{c\in C} [\log (\tilde \pi_c) - \frac{1}{2} (x-\mu_c)^T \Sigma^{-1}_c (x-\mu_c)]
\end{equation}
Where $\tilde \pi_c = \frac{1}{(2\pi)^{D/2} |\Sigma_c|^{1/2}} \pi_i$

\begin{equation}
P(c | x) = \pi_c \frac{1}{(2\pi)^{D/2} |\Sigma_c|^{1/2} } \exp(\frac{-1}{2} (x-\mu_c)^T \Sigma^{-1} (x - \mu_c))\frac{1}{P(x)} 
\end{equation}
Where $P(x)$ is given by
\begin{equation}
\begin{split}
P(x) &= \sum_{c'\in C}^{} P(x | c')P(c') \\
&= \sum_{c'\in C}^{} \mathcal N(x; \mu_c, \Sigma_c) P(c')
\end{split}
\end{equation}

\paragraph{Calculating $\mu_c, \Sigma_c$} 

The maximum likelihood estimation of these are as follows: 
\begin{equation}
\hat\mu_c = \frac{1}{n} \sum_{i=1}^{n} x_i \,\, \forall x_i \in c
\end{equation}
\begin{equation}
\hat \Sigma_c^{ML} = \frac{1}{n} \sum_{i=1}^{n} (x_i - \hat \mu_c)(x_i - \hat\mu_c)^T\,\, \forall x_i \in c
\end{equation}
\textbf{PROBLEM:} $\hat \Sigma_c^{ML}$ is a biased estimator. For the $D=1$ case, $\mathbb E[\hat \sigma_c^{ML}] = \frac{n-1}{n} \sigma^2$. To correct, we use the following:
\begin{equation}
\hat \Sigma_c^{corrected} = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \hat \mu_c)(x_i - \hat\mu_c)^T\,\, \forall x_i \in c
\end{equation}

The primary problem with these sorts of classifiers is that $\Sigma$ has high dimensionality. If we use the same $\Sigma$ for all classes, we get LDA. If we force $\Sigma_c$ to be diagonal, we get a naive Bayesian classifier.





% TODO: March 1 lecture videos notes.

\section{General Bayesian Inference on Gaussian Vectors}

\paragraph{Given: } We know that $\vec z \sim \mathcal N(\vec \mu, \Sigma)$. We know the value of only part of the $\vec z$ vector, and want to determine the probability distribution and ML/MLE/MMSE estimation for the remaining indices we don't know. 

\begin{equation}
\vec z = \begin{bmatrix}
\vec x \\
\vec y \\
\end{bmatrix}
\end{equation}

We are given the value of $\vec y$ and want to know $\vec x$.

\paragraph{Key Tools: } 
\begin{equation}
\Sigma = \begin{bmatrix}
\Sigma_{xx} & \Sigma_{xy} \\
\Sigma_{yx} & \Sigma_{yy}
\end{bmatrix}
\end{equation}
Where $\Sigma_{xy} = \mathbb E[(x - \mu_x)(x - \mu_x)^T]$.

\begin{itemize}
\item \textit{The maximum of a Gaussian is at the mean!} 
\item $f_{x|y} \sim \mathcal N$
\item $\hat x_{MAP} = \hat x_{MMSE,LMS} = \mathbb E[\vec x | \vec y]$
\end{itemize}


\paragraph{Results: } 

\begin{equation}
f_{x|y}(x|y) \sim \mathcal N(\mu_{x|y}, \Sigma_{x|y})
\end{equation}
Where 
\begin{equation}
\mu_{x|y} = \mu_x + \Sigma_{xy} \Sigma_{yy}^{-1} (y-\mu_y)
\end{equation}
\begin{equation}
\Sigma_{x|y} = \Sigma_{xx} - \Sigma_{xy}\Sigma^{-1}_{yy}\Sigma_{yx}
\end{equation}

Which is pretty cool, especially considering that $\hat x_{MAP, MMSE} = \mu_{x|y}$ is now linear with $\vec y$. We can think of $\Sigma_{x|y}$ as the ``remaining uncertainty'' in $\vec x$ after we receive information $\vec y$. 


\section{Linear Gaussian Systems}

\paragraph{Given: } $P(x) \sim \mathcal N(\mu_x, \Sigma_x)$. $y = Ax+b+z$ where
\begin{itemize}
\item $A$ is a known matrix.
\item $b$ is a known vector.
\item $z\sim \mathcal N(0, \Sigma_z)$
\item $x,z$ are independent.
\end{itemize}

\paragraph{Goal: } We observe $y$. What are our estimates $\hat x_{MAP}$, $\hat x_{MMSE}$ that correspond to this particular $y$?

\paragraph{Solution: } Unsurprisingly, $x,y$ have Gaussian distributions.
\begin{equation}
\hat x_{MAP, MMSE} = \mu_x + \Sigma_{xy}\Sigma_{yy}^{-1}(y-\mu_y)
\end{equation}
Where
\begin{equation}
\mu_y = A\mu_x + b
\end{equation}
\begin{equation}
\Sigma_{xy} = \Sigma_x A^T
\end{equation}
\begin{equation}
\Sigma_{yy} = A\Sigma_xA^T + \Sigma_z
\end{equation}
\begin{equation}
\Sigma_{x|y} = (\Sigma_x^{-1} + A^T\Sigma_z^{-1}A)^{-1}
\end{equation}

















































\end{document}
