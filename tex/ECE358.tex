\documentclass[a4paper,12pt]{report}
\usepackage{color}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{listings}
\usepackage{media9}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{mathtools} 
\usepackage{bbm}
% \usepackage{algorithm2e}
% \usepackage[ruled,vlined]{algorithm2e}
\usepackage[vlined,ruled,commentsnumbered]{algorithm2e}

% Some nice shortcuts.
\newtheorem{theorem}{Theorem}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\begin{document}

\title{ECE358: Foundations of Computing}
\author{Aman Bhargava}
\date{September-December 2020}
\maketitle

\tableofcontents

\section{Introduction and Course Information}

This document offers an overview of the ECE358 course. They comprise my condensed course notes for the course. No promises are made relating to the correctness or completeness of the course notes. These notes are meant to highlight difficult concepts and explain them simply, not to comprehensively review the entire course.

\paragraph{Course Information}
\begin{itemize}
\item Professors: Prof. Andreas Veneris and Prof. Zissis Poulos
\item Course: Engineering Science, Machine Intelligence Option
\item Term: 2020 Fall
\end{itemize}

\chapter{Math Review}

\section{Logarithms}

\paragraph{Logarithm Rules: } 
\begin{itemize}
\item \textbf{Definition: } $a = b^c$ iff $log_b a = c$.
\item $a = b^{\log_b a}$.
\item $\log_c(ab) = \log_c a + log_c b$.
\item $\log_c(a^n) = n\log_c(a)$.
\item $\log_b(a) = \log_a(b)$.
\item $\log(1/a) = -\log a$.
\item $\log(a/c) = \log a - \log c$.
\item $a^{\log n} = n^{\log a}$.
\end{itemize}

\paragraph{Variations on Logarithm} 

\begin{equation}
\log^{(i)} n = \begin{cases}
n & \text{iff } i = 0 \\
\log(\log^{(i)} n) & \text{otherwise}
\end{cases}
\end{equation}

\begin{equation}
\log^* n = \begin{cases}
0 & \text{if } n \leq 1 \\
1 + log^*(\log n) & \text{otherwise}
\end{cases}
\end{equation}


\section{Sequences and Series}

\begin{theorem}{Fibonnaci Definition:}
\begin{equation}
F_i = F_{i-1} + F_{i-2}
\end{equation}

Where $F_0 = 0$, $F_1 = 1$.

\begin{equation}
F_i = \frac{\phi^i - \hat \phi^i}{\sqrt{5}}
\end{equation}

Where $\phi = \frac{1+\sqrt{5}}{2}$, $\hat \phi = \frac{1-\sqrt 5}{2}$
\end{theorem}


\begin{theorem}{Arithmetic Series:}
\begin{equation}
\sum_{i=1}^{n} = \frac{n(n+1)}{2} = \Theta(n^2)
\end{equation}
\end{theorem}


\begin{theorem}{Geometric Series:}
\begin{equation}
\sum_{k=0}^{n} x^k = \frac{x^{n+1}-1}{x-1} 
\end{equation}
\end{theorem}


\begin{theorem}{Infinite Series:}
\begin{equation}
\sum_{k=0}^{\infty} x^k = \frac{1}{1-x} \text{ iff } |x| < 1
\end{equation}
\end{theorem}


\begin{theorem}{Telescoping Series:}
\begin{equation}
\sum_{i=1}^{n} a_i - a_{i-1} = a_n - a_0 \\
\sum_{i = 1}^{n} a_i - a_{i+1} = a_0 - a_n
\end{equation}
\end{theorem}


\section{Combinatorics}

\begin{theorem}{Binomial Coefficient:}
\begin{equation}
(x+y)^r = \sum_{i = 0}^{r} {r\choose i} x^i y^{r-i}
\end{equation}

Where $r\choose i$ is the binomial coefficient which equals $$\frac{r!}{i!(r-i)!}$$

\end{theorem}


% TODO: Get the rest of the information from ECE286 notes. 


\chapter{Asymptotics}

\paragraph{What are Asymptotics? } Analysis method for performance and complexity of an algorithm (space/memory, time/clock cycle complexity).

\begin{itemize}
\item Tight Bound: $\Theta()$.
\item Worst Case: $O()$.
\item Best Case: $\Omega()$
\item Average \& Expected Case: \textit{Randomized + probabilistic analysis. Not a focus for the course.} 
\item Amortized Case: Discused later on. ``Average worst case''.
\end{itemize}


\paragraph{Useful Facts} 
\begin{itemize}
\item $\Theta$ bound is not always possible to find. 
\item \textbf{Transitivity: } if $f(n) = \Theta(g(n))$ and $g(n) = \Theta(h(n))$, then $f(n) = \Theta(h(n))$.
\item \textbf{Symmetry: } $f(n) = \Theta(g(n))$ if and only if $g(n) = \Theta(f(n))$.
\item \textbf{Transpose: } $f(n) = O(g(n))$ if and only if $g(n) \Omega(f(n))$.
\item $n^a \in O(n^b)$ iff $a \leq b$.
\item $\log_a(n) \in O(\log_b(n)) \, \forall a, b$.
\item $c^n \in O(d^n)$ iff $c\leq d$.
\item If $f(n) \in O(f'(n))$ and $g(n) \in O(g'(n))$ then $$f(n)\cdot g(n) \in O(f'(n)\cdot g'(n))$$
$$f(n) + g(n) \in O(\max(f'(n), g'(n))$$
\end{itemize}




\begin{theorem}{Big O Definition:}
$f(n) = O(g(n))$ if and only if: 
There exists $c > 0$ and $n_0 > 0$ such that
\begin{equation}
0 \leq f(n) \leq cg(n)\,\forall n \geq n_0 
\end{equation}
\end{theorem}


\begin{theorem}{Big $\Omega$ Definition:}
\paragraph{``Lower Bound'' Big $\Omega$ Definition: } $f(n) = \Omega(h(n))$ if and only if:
There exists $c_1, n_1 > 0$ such that 
\begin{equation}
0 \leq c_1 h(n) \leq f(n)\, \forall n \geq n_1
\end{equation}
\end{theorem}



\begin{theorem}{Big $\Theta$ Definition:}
\paragraph{``Tight Bound'' Big $\Theta$ Definition: } $f(n) = \Theta(g(n))$ if and only if:

There exists $c_1, c_2, n_0 > 0$ such that 

\begin{equation}
0 \leq c_1 g(n) \leq f(n) \leq c_2 g(n) \, \forall n > n_0
\end{equation}
\end{theorem}


\chapter{Graphs}

\section{Definitions}

\begin{theorem}{Graph Definition:}
A \textbf{graph} is a collection of \textbf{vertices} and \textbf{edges} represented by 
\begin{equation}
G = (V,E)
\end{equation}
Graphs can be:
\begin{itemize}
\item Directed or Undirected.
\item Weighted or Unweighted.
\end{itemize}

\end{theorem}


\paragraph{Path: } A sequence of \textbf{edges} between adjacent vertices. 
\begin{itemize}
\item \textbf{Simple Path: } No vertex is repeated in the path.
\item \textbf{Cycle: } A path that starts and ends with the same vertex.
\item A \textbf{connected} graph has a path between any two vertices (else it is \textbf{disconnected})
\end{itemize}


\paragraph{Bipartite Graphs: } A graph is \textbf{bipartite} if and only if vertices $V$ can be divided into $V_1, V_2$ such that:
\begin{enumerate}
\item $V_1 \cap V_2 = \emptyset$.
\item $V_1 \cup V_2 = V$.
\item Adjacencies exist only between elements and $V_1$ and $V_2$ (i.e., vertices within each set are disconnected from eachother, but are connected to elements from the other set).
\end{enumerate}



\paragraph{Vertex Degree } is the number of \textbf{edges} adjacent to a vertex (includes incoming and outgoing edges).
\begin{itemize}
\item \textbf{In-degree} is the number of incoming edges.
\item \textbf{Out-degree} is the number of outgoing edges. 
\end{itemize}

\paragraph{Clique: } For all $v_1, v_2\in V$, there exists an edge connecting them (i.e. a fully-connected set of vertices). 


\paragraph{Representations for Graphs: } We either use an \textbf{adjacency matrix} or an \textbf{adjacency matrix}.

\begin{itemize}
\item \textbf{Adjacency List: } Each element in the list represents a vertex and ``has'' a collection of \textbf{pointers} connecting them to other elements of the list. 
\begin{itemize}
\item \textbf{Time Complexity: } $O(n)$ -- Determining if $E_{v_1,v_2}$ exists would, at worst, require you to check $n$ pointers arising from vertex $v_1$ and/or $v_2$. 
\item \textbf{Space Complexity: } $O(|E|)$ -- with worst case that $|E| = n^2$ for clique. 
\end{itemize}

\item \textbf{Adjacency Matrix: } Matrix contains weights connecting $v_i$ to $v_j$ at index $M_{i,j}$.
\begin{itemize}
\item \textbf{Time Complexity: } $O(1)$ -- Just check address $M_{i,j}$ and $M_{j,i}$.
\item \textbf{Space Complexity: } $O(n^2$ every time. 
\end{itemize}
\end{itemize}





\section{Graph Traversals}

\textit{How do we choose an order to traverse all nodes in a graph?} 

\subsection{Breadth First Search}

\paragraph{Key Ideas: } 
\begin{itemize}
\item From a given point, we traverse all immediate neighbours. 
\item Only then do we go to second-order neighbours of the original point.
\item Implemented with a \textbf{Queue} (first-in-first-out).
\end{itemize}

\paragraph{Breadth First Search Algorithm: } Given $G=(V,E,s$) where $s$ is the starting vertex,
\begin{enumerate}
\item Initialize $d[u] = \infty\,\forall u\in V$. This is the \textbf{distance} for each vertex.
\item Let $Q$ be an empty queue, and \textbf{enqueue} vertex $s$.
\item While $Q$ is not empty:
\begin{enumerate}
\item $u = $ dequeue$(Q)$.
\item For $v\in adj(u)$:
\begin{enumerate}
\item if $d[v] = \infty$, let $d[v] = d[u]+1$ and \textbf{enqueue} $v\to Q$.
\end{enumerate}
\end{enumerate}
\end{enumerate}

\paragraph{Performance: } Time complexity is $O(|V|+|E|)$.



\subsection{Depth First Search}

\paragraph{Key Ideas: } 
\begin{itemize}
\item \textbf{Input: } $G = (V,E)$ (no source vertex this time).
\item \textbf{Output: } 
\begin{enumerate}
\item $d[v]$: Time taken to \textbf{discover} vertex $v$ (in number of algorithm ``steps'').
\item $f[v]$: Time taken to \textbf{finish} with vertex $v$ (meaning to exhaust all `down stream' vertices).
\item $\Pi[v]$: Predecessor of $v$ (i.e. the vertex immediately before it in the search).
\end{enumerate}
\end{itemize}

\paragraph{Color Coding: } 
\begin{itemize}
\item \textbf{White: } Node is as of yet undiscovered.
\item \textbf{Gray: } Node has been discovered, but the descendants have not all been discovered (node is not ``exhausted'').
\item \textbf{Black: } The algorithm has traversed the node AND all its descendants. 
\end{itemize}

\begin{algorithm}[H]
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\SetAlgoLined
\Input{$G = (V,E)$}
\Output{List of discovery times, finishing times, and predecessors for each $v\in V$.}
	\BlankLine
	\BlankLine
	%\nlset{\color{blue}$O(n)$}Some order $n$ operation\;
	$\text{color}[u] = \text{White}\,\forall u\in V$\;
	$\text{time} = 0$\;
	\For{each $u\in V$}{
		\If{$\text{color}[u] = \text{White}$}{
			$\text{DFS-Visit}(u)$\;
		}
	}	
	\caption{Depth First Search Algorithm.}
\end{algorithm}


\begin{algorithm}[H]
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\SetAlgoLined
\Input{Vertex $u$}
\Output{Recursively traverses nodes depth-first.}
	\BlankLine
	\BlankLine
	%\nlset{\color{blue}$O(n)$}Some order $n$ operation\;
	$\text{color}[u] \leftarrow \text{Gray}$\;
	$\text{time} += 1$\;
	$d[u] = \text{time}$\;

	\For{each $v\in \text{adj}(u)$}{
		\If{$v = \text{White}$}{
			$\text{DFS-Visit}(v)$\;
		}
	}
	
	$\text{color}[u] \leftarrow \text{Black}$\;
	$\text{time} += 1$\;
	$f[u] \leftarrow \text{time}$\;

	\caption{DFS-Visit Algorithm (subroutine for DFS Search)}
\end{algorithm}


\paragraph{Edge Classification: } \textbf{DFS} Produces one or more \textbf{trees}, leading to the following classification.
\begin{enumerate}
\item \textbf{Tree Edges: } Edges found during DFS exploration.
\item \textbf{Back Edge: } $(u,v)$ when $u$ is a descendant of $v$.
\item \textbf{Forward Edge: } $(u,v)$ when $v$ is a descendant of $u$ \textbf{but} $(u,v)$ is not a tree edge.
\item \textbf{Cross Edge: } Any other type of edge. 
\end{enumerate}


\begin{theorem}{White Path Theorem: }
$v$ is a descendant of $u$ if, and only if: At time $d[u]$, there exists a \textbf{white-only} path from $v\to u$.
\end{theorem}

\begin{theorem}{Parenthesis Theorem: }
For all $u, v\in V$: Exactly \textbf{one} of the following holds:
\begin{enumerate}
\item $d[u] < f[u] < d[v] < f[v]$ or $d[v] < f[v] < d[u] < f[u]$. That is, \textbf{neither $u, v$ are descended from eachother}.
\item $d[u]< d[v] < f[v] < f[u]$. That is, $v$ is a descendant of $u$.
\item $d[v] < d[u] < f[u] < f[v]$. That is, $u$ is a descendant of $v$.
\end{enumerate}
\end{theorem}

\begin{theorem}{Depth First Search Edge Types: }
The depth-first search of \textbf{undirected graph} $G$ yields only \textit{tree edges} and \textit{back edges} .
\end{theorem}


\subsection{Topological Sort}

\paragraph{Key Idea: } All DAG (directed, acyclic graphs) have \textit{some} amount of \textbf{implied order}. Think of it like a dependency graph. We use topological sort to turn \textbf{partial order} $\to$ \textbf{total order} (that is, an absolute list).


\begin{algorithm}[H]
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\SetAlgoLined
\Input{DAG $G = (V,E)$}
\Output{Total ordering of elements of graph.}
	\BlankLine
	\BlankLine
	%\nlset{\color{blue}$O(n)$}Some order $n$ operation\;
	Call $\text{Depth First Traversal}(V,E)$\;
	Return the vertices in \textbf{decreasing order} of \textbf{finish time}\;
	\caption{Topological Sorting Algorithm.}
\end{algorithm}




\section{Minimum Spanning Trees}

\paragraph{Key Idea: } Given a weighted graph $G = (V, E)$, we want a tree $G' = (E', V)$ where $E' \subseteq E$ such that there is a path between all vertices \textbf{and} $\sum_{e\in E'} e$ is minimized.
\begin{itemize}
\item $|E'| = |V|-1$.
\item There are \textbf{no cycles} (hence ``tree'').
\item It may or may not be a \textbf{unique} solution.
\end{itemize}

\subsection{Generic Minimum Spanning Tree Approach}

\paragraph{High-Level Description: } 
\begin{itemize}
\item Set $A$ is instantiated as empty.
\item Edges from the graph are added to $A$, ensuring \textbf{loop invariant} $A\subset\text{Some Minimum Spanning Tree}$. 
\item Edge $(u,v)$ is \textbf{safe} iff $A \cup \{(u,v)\}$ is also a subset of some minimum spanning tree.
\end{itemize}

\paragraph{Generic MST Algorithm: } Given graph $G$ with weights $w$,
\begin{enumerate}
\item Let $A$ be an empty set. 
\item While $A$ is not a spanning tree: 
\begin{enumerate}
\item Find a \textbf{safe} edge $(u,v)$.
\item $A \leftarrow A \cup (u,v)$.
\end{enumerate}
\end{enumerate}

\paragraph{Terminology: } Relating to how to pick a \textbf{safe} edge.
\begin{itemize}
\item Let $S$ represent the set of vertices currently ``covered'' by edges in $A$, our current working minimum spanning tree.
\item We can partition $V$ into set $S$ and $V-S$.
\item Edge $(u,v)$ \textbf{crosses} the \textbf{cut} $(S,V-S)$ if one of $u,v$ is in $S$ and the other is in $V-S$.
\item A cut \textbf{respects} $A$ iff no edge in $A$ \textbf{crosses} the cut. 
\item The \textbf{light edge} with respect to cut $(S,V-S)$ is the lowest weight edge that \textbf{crosses} the cut. 
\item \textbf{The light edge is the safe edge}.
\end{itemize}



\subsection{Prims Algorithm}

\begin{algorithm}[H]
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\SetAlgoLined
\Input{Graph $G=(V,E)$, weights $w$, root node $r$.}
\Output{Minimum spanning tree of $G$.}
	\BlankLine
	\BlankLine
	%\nlset{\color{blue}$O(n)$}Some order $n$ operation\;
	$Q \leftarrow \{\emptyset\}$\;
	\For{each $u\in V$}{
		$\text{key}[u] \leftarrow \infty$\;
		$\Pi[u] \leftarrow \text{NIL}$\;
		$\text{Insert} u\to Q$\;
	}

	Set key of root node $r$ to $0$ in $Q$.

	\While{$Q \neq \{\emptyset\}$}{
		$u\leftarrow \text{Pop-Minimum}(Q)$\;
		\For{each $v\in \text{Adj}(u)$}{
			\If{$v\in Q$ and $w(u,v) \leq \text{key}(v)$}{
				$\Pi[v] \leftarrow u$\;
				Set the key of $v$ in $Q$ to $w(u, v)$\;
			}
		}
	}

	\caption{Prim's Algorithm}
\end{algorithm}

\paragraph{Complexity: } Time complexity is $O(|E| + |V|\log |V|)$ when using Fibonnaci heaps. 








\section{Shortest Paths}

\paragraph{Goal: } Find path of minimum total weight between two vertices in a graph. Shortest path from $u\to v$ is represented by $\delta(u,v)$. It's the set of vertices from one to the other. 

\paragraph{Variants of Shortest Paths Problem: } 
\begin{enumerate}
\item Single Source Shortest Paths (SSSP).
\item Single Destination Shortest Paths.
\item Single Pair -- \textit{most efficient method is to just run SSSP}.
\item All Pairs Shortest Paths.
\end{enumerate}

\paragraph{Key Algorithms: } 
\begin{enumerate}
\item Dijkstra's -- for no negative weights.
\item Bellman-Ford -- when there are negative weights.
\item Difference Constraints -- a surprising application of shortest paths. 
\end{enumerate}


\paragraph{Properties and Assumptions of Shortest Paths: } 
\begin{enumerate}
\item No negative weight cycles allowed accessible from source node. 
\item \textbf{Optimal Substructure: } If $p$ is in the optimal path from $u\to v$ $\delta(u,v)$, then sub-path $u\to p$ is the optimal path $\delta(u,p)$.
\item A shortest path will \textbf{never contain a cycle}.
\begin{itemize}
\item Negative weight cycles aren't allowed (per above).
\item Positive weight cycles would be a pointless cost expenditure. 
\item 0-value cycles are redundant (by convention, we just skip them). 
\end{itemize}
\end{enumerate}

\subsection{Djikstra's Algorithm}

\paragraph{Key Points: } 
\begin{itemize}
\item No negative weight edges allowed.
\item We let $S$ be the set of vertices with known shortest path weight. 
\item Priority queue $Q$ is the remaining set $V-S$ of vertices. 
\item Priority is dictated by $d[v]$.
\item \textbf{Time Complexity: } If we use a binary heap, we get $$O(|E| \log |V|)$$ though Fibbonacci heap is faster. 
\end{itemize}

\begin{algorithm}[H]
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\SetAlgoLined
\Input{Graph $G = (V, E)$, weights $w$ and starting node $s$.}
\Output{Shortest paths for each $v\in V$ starting at $s$.}
	\BlankLine
	\BlankLine
	%\nlset{\color{blue}$O(n)$}Some order $n$ operation\;
	$\text{Init-Single-Source}(V, s)$\;
	$S\leftarrow \{\emptyset\}$\;
	$Q\leftarrow V$\;
	
	\While{$Q\neq \{\emptyset\}$}{
		$u\leftarrow \text{Extract-Min}(Q)$\;
		$S \leftarrow S \cup \{u\}$\;
		\For{each $v\in \text{adj}(u)$}{
			$\text{Relax}(u, v, w)$;
		}
	}

	\caption{Dijkstra's Algorithm}
\end{algorithm}

\paragraph{Subroutines: } 
\begin{enumerate}
\item \textbf{Init-Single-Source}$V,s$: Let $d[s] = 0$. Then, for each vertex $v\in V$,
\begin{itemize}
\item $d[v] \leftarrow \infty$.
\item $\Pi[v] \leftarrow \text{NIL}$.
\end{itemize}

\item \textbf{Relax}$(u,v,w)$:
\begin{itemize}
\item If $d[v] > d[u] + w(u,v)$, that means that it's more efficient to reach $v$ through $u$ than what it's currently using.
\item We set $d[v] \leftarrow d[u] + w(u,v)$.
\item We set $\Pi[v] \leftarrow u$.
\end{itemize}




\end{enumerate}

























\chapter{Trees}

\section{Definitions}

\begin{theorem}{Tree Definition:}
A \textbf{tree} is a \textbf{connected, acyclic, undirected} graph.
\begin{itemize}
\item \textbf{Root node} is usually defined.
\item \textbf{Parent node} is the `next node up' going toward the root.
\item \textbf{Child node} is one of the `next node(s) down' going away from the root.
\item \textbf{Binary tree: } $\leq 2$ children per node.
\item \textbf{Depth of node: } Length of path from \textbf{root $\to$ node}.
\item \textbf{Height of node: } Number of edges on the \textbf{longest path} from the node $\to$ leaf. 
\item \textbf{Complete $k$-ary tree: } Every \textit{internal node} has $k$ children and all leaves are at the same depth.
\end{itemize}
\end{theorem}


\begin{theorem}{One-for-all Tree Theorem:}
\textbf{If one of these is true, they all are: } 
\begin{itemize}
\item $G$ is a tree.
\item Every pair of vertices $v_1, v_2\in G$ is connected by a \textbf{unique, simple path}.
\item $G$ is disconnected, but becomes disconnected when one edge is removed.
\item $G$ is connected with $|E| = |V|-1$.
\item $G$ is acyclic.
\item If an edge is added $G$ \textbf{becomes cyclic}.
\end{itemize}
\end{theorem}



\section{Binary Search Trees}

\begin{theorem}{Binary Search Tree Definition:}
For any node $x$ of a binary search tree: 
\begin{itemize}
\item If $y\in $ left sub-tree: $y.key \leq x.key$.
\item If $y\in $ right sub-tree: $y.key \geq x.key$.
\end{itemize}
\end{theorem}


\paragraph{Traversals: } 
\begin{enumerate}
\item \textbf{In-Order: } LNR -- Actually prints them from least to greatest.
\item \textbf{Pre-Order: } NLR
\item \textbf{Post-Order: } LRN
\end{enumerate}


\subsection{Querying Binary Search Trees}

We can perform minimum, maximum, successor, predecessor in $O(h)$ time where $h$ is the height of the tree.

\paragraph{Tree Search: } Given a node $x$ and value we are looking for $k$:
\begin{enumerate}
\item If $x$.key is $k$ or $x$ is NIL: return $x$. \textit{We either found it or it wasn't there}.
\item If $x$.key $< k$: Run \textbf{Tree Search} on the left sub-tree.
\item Otherwise, run \textbf{Tree Search} on the right sub-tree.
\end{enumerate}

\paragraph{Iterative Tree Search: } Same input/output as above: \textbf{Repeat while $x$.key is not $k$ and NIL} 
\begin{enumerate}
\item If $x$.key $<k$: $x = x$.left.
\item Else, $x = x$.right.
\item After break condition above, return $x$.
\end{enumerate}


\paragraph{Minimum Tree Search: } To find the minimum, keep setting $x = x$.left while $x$.left isn't NIL.


\paragraph{Tree Successor: } To find the successor of node $x$: 
\begin{enumerate}
\item If $x$ has a right child: Call \textbf{Tree Minimum} on $x$.right.
\item Else: 
\begin{enumerate}
\item $y = x$.parent
\item While $y \neq$ NIL and $x == y$.right:
\begin{enumerate}
\item $x = y$
\item $y = y.parent$
\end{enumerate}
\item return $y$
\end{enumerate}
\end{enumerate}


\paragraph{Tree Insert: } To insert $z$ into BST $x$, repeatedly compare $z$ to nodes of $x$, moving right/left according to result of comparison. Eventually, you will add $z$ as a leaf node. 

\paragraph{Tree Deletion: } To delete node from $z$ from BST: 
\begin{enumerate}
\item $z$ has \textbf{no children} $\to$ modify $z$.parent.child = NIL.
\item $z$ has \textbf{one child} $\to$ modify z.parent.child $\leftarrow$ $z$.child.
\item $z$ has \textbf{two children}:
\begin{enumerate}
\item Let $y$ = Tree Successor($z$) [must be from right sub-tree of $z$]
\item Convenient case: $y = z$.right. In this case, just replace $z$ with $y$.
\item Replace $z \leftarrow y$, ensuring $y$ keeps its right sub-tree.
\item $z$.right $\to$ $y$.right.
\item $z$.left $\to$ $y$.left.
\end{enumerate}
\end{enumerate}


\section{Red-Black Trees}

\begin{theorem}{Red Black Tree Definition:}
A red-black tree is a \textbf{binary search tree} where each node has a ``color'' of either \textbf{red or black}.Each BST operation has additional rules that relate to color, with the end results being:
\begin{itemize}
\item No path from the \textbf{root} to a \textbf{leaf} is more than \textbf{two times the shortest path}.
\item The tree is approximately balanced and therefore is efficient.
\end{itemize}

\paragraph{Properties of Red-Black Tree} 
\begin{enumerate}
\item All nodes are either \textbf{red or black}.
\item The root node is black.
\item All leaves are black.
\item Red nodes have black children.
\item All simple paths from a node to descendant leaves have \textbf{the same numberof black nodes}.
\end{enumerate}

\paragraph{Resultant Properties} 
\begin{enumerate}
\item bh$(x)$ is the \textbf{``black height''} of node $x$ (numberof black nodes from (but not including) $x$ to a leaf.
\item Red-black tree with $n$ internal nodes has height $\leq 2\log(n+1)$.
\item All leaves are set to $T$.nil values instead of regular NIL.
\end{enumerate}
\end{theorem}

\paragraph{Easy Red-black Tree Operations: } These are exactly the same as BST operations. They all run in $O(\log n)$ time, too.
\begin{enumerate}
\item Search.
\item Minimum.
\item Maximum.
\item Successor.
\item Predecessor. 
\end{enumerate}


\paragraph{Rotations: } Rotations swap two nodes while maintaining the BST properties. You can visualize or ``feel'' it as taking the edge connecting two nodes and physically \textbf{twisting} it either right or left for their respective rotation.

\paragraph{Red-Black Insert Fixup: } This algorithm reinstates the red-black properties of the tree around a given recently-inserted node $z$. 

\paragraph{Red-Black Insertion: } To insert node $z$...
\begin{enumerate}
\item Perform conventional BST insertion of $z$, setting the new node's children to $T$.nil instead of NIL.
\item Set $z$'s color to RED.
\item Call \textbf{Red-Black Insert Fixup}$(T, z)$.
\end{enumerate}

\paragraph{Red-Black Transplant: } Same function as before, just replace 'NIL' with $T$.nil.

\paragraph{Red-Black Deletion: } Deletion is substantially more involved and uses a particular \textbf{Red-Black Delete Fixup} function. 










\chapter{Proof Methods}

\section{Induction}

\paragraph{Basic Idea: } To prove by induction, we show that the statement holds for some base case $n = 1$, then show that the statement holding for aritrary $n$ implies that the statement holds for $n+1$. That concludes the proof. 

\paragraph{Basis: } We prove that the statement holds for some set values of $n$ (usually $n = 1$ or $n = 0,1,...,4$).

\paragraph{Hypothesis: } We hypothesize that the thing we are trying to prove is true. 

\paragraph{Inductive Step: } We ``plug in'' the $n+1$ case to the hypothesis and show that it boils down to the $n$ case and some algebraic equivalence. Then you can make the claim that it holds for all $n$.


% TODO: Add an illustrative example. 


\section{Contradiction}

\paragraph{Basic Idea: } Given a true or false proposition $P$, we assume that $\neg P$ (``not $P$'') holds. After some clever algebra and symbol-shunting, we get a contradiction. This implies that $P$ must hold instead. 

\paragraph{Illustrative Example: } $P$: If $x^2 - 5x + 4 < 0$, then $x > 0$.
\begin{itemize}
\item To prove $P$, we \textbf{assume towards a contradiction} (ATaC) that $\neg P$ holds. That is, we assume that if $x^2 - 5x + 4 < 0$ then $x \leq 0$. 
\item But then: 
\begin{equation}
\begin{split}
x^2 &< 5x-4 \\
x^2 &< 0 \\ 
\end{split}
\end{equation}
Results in a contradiction! Therefore $P$ must hold. 
\end{itemize}



\section{Master Theorem}

This provides a ``hammer'' to prove the time complexity of recurrent functions.
\begin{theorem}{The Master Theorem:}
Let $a \geq 1$ and $b \geq 1$ and $f(n)$ be some function.
\begin{equation}
T(n) = aT(\frac{n}{b}) + f(n)
\end{equation}

\paragraph{Case 1: } $f(n) = O(n^{\log_b a - \epsilon})$ for some $\epsilon > 0$. Then $$T(n) = \Theta(n^{\log_b a})$$

\paragraph{Case 2: } $f(n) = \Theta(n^{\log_b a})$. Then $$T(n) = \Theta(n^{\log_b a} \log n)$$

\paragraph{Case 3: } $f(n) = \Omega(n^{\log_b a + \epsilon})$ for some $\epsilon > 0$ \textbf{AND} $af(n/b) \leq c f(n)$ for $0 < c < 1$. Then $$T(n) = \Theta(f(n))$$.
\end{theorem}


% TODO: Add notes on merge sort.






\section{Substitution}

\textbf{Substitution} is a method for determining the \textbf{closed from} runtime of an algorithm via induction.

\paragraph{Example -- Mergesort: } The runtime for mergesort is recursively defined as $T(n) = 2T(\ceil{n/2}) + n$.
\begin{enumerate}
\item We start by \textbf{guessing} $T(n) = O(n\log n)$. 
\item We \textbf{hypothesize} that $T(n) = O(n\log n)$ for all cases $\leq n$, meaning that
$$T(n/2) \leq c \floor{n/2}\log \floor{n/2}$$
\item \textbf{Inductive step:} We prove that $$T(n) \leq cn\log n$$ Which is pretty obvious from there. 
\end{enumerate}



\chapter{Sorting Algorithms}

\section{Mergesort}
df
% TODO: Add pseudocode for heapsort.

\section{Heapsort}
df
\subsection{Heaps}

\begin{theorem}{Heap Definition:}
A heap is an \textbf{array} representing a \textbf{binary tree} with the following properties:
\begin{itemize}
\item For \textbf{max heap}: $A[\text{parent}(i)] \geq A[i]$.
\item For \textbf{min heap}: $A[\text{parent}(i)] \leq A[i]$.
\item \textbf{Height} of tree is $\Theta(\log n)$ \textit{(i.e. number of edges to get from root to leaf node)}. 
\end{itemize}

Parent-child relationships are as follows:
\begin{itemize}
\item $\text{Parent}(i) = \floor{i/2}$ (Parent node of $i$).
\item $\text{Left}(i) = 2i$ (Left child).
\item $\text{Right}(i) = 2i+1$ (Right child).
\end{itemize}
\end{theorem}

\subsection{Heap Algorithms}

\textbf{Max-Heapify} is an algorithm that sorts array $A$ into a heap at index $i$, assuming that right and left sub-trees satisfy the heap property at $i$. 

\begin{algorithm}[H]
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\SetAlgoLined
\Input{Array $A$, index $i$. Left and right sub-trees of $i$ should already satisfy heap property.}
\Output{$A$ is sorted in place to satisfy heap property.}
    \BlankLine
    \BlankLine
    %\nlset{\color{blue}$O(n)$}Some order $n$ operation\;
    $l$ = left($i$)\;
    $r$ = right($i$)\;

    Let largest = $\max(l,r,i)$.
    
    \eIf{$i \neq$ largest}{
        Exchange $i$, largest\;
        MaxHeapify($A$, largest)\;
    }{}
    \caption{Max Heapify}
\end{algorithm}

\paragraph{Algorithm Complexity: } $O(1)$ Space complexity (sorts in place), $O(\log n)$ time complexity.


\paragraph{Build Heap } is an algorithm that sorts a completely unsorted array $A$ into a heap. 

\begin{algorithm}[H]
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\SetAlgoLined
\Input{Unsorted list $A$}
\Output{Heap $A$}
	\BlankLine
	\BlankLine
	%\nlset{\color{blue}$O(n)$}Some order $n$ operation\;
	\For{$i = \floor{|A|/2}:1$}{
		MaxHeapify($A,i$)\;	
	}
	
	\caption{Build Heap}
\end{algorithm}

\paragraph{Algorithm Complexity: } $O(1)$ Space Complexity, $O(n)$ time complexity.



\paragraph{Heap Sort } is a sorting algorithm that runs in $O(n\log n)$ time. 

\begin{algorithm}[H]
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\SetAlgoLined
\Input{Unsorted array $A$}
\Output{Sorted array $A$}
	\BlankLine
	\BlankLine
	%\nlset{\color{blue}$O(n)$}Some order $n$ operation\;
	%\nlset{\color{blue}$O(n)$} BuildMaxHeap(A)
	\For{$i = |A|:2$}{
		exchange $A[i]$, $A[1]$\;
		$A$.heapsize -= 1\;
 		MaxHeapify($A,1$)\;
	}
	\caption{Heap Sort Algorithm}
\end{algorithm}

\paragraph{Algorithm Complexity: } $O(1)$ Space Complexity, $O(n)$ time complexity.



\section{Quicksort}

\paragraph{Key Facts: } Quick sort sorts in $\Theta(n^2)$ in the worst case and $\Theta(n\lg n)$ in the average case. The factors on the average case are small, so it ends up being very practical.

\paragraph{Algorithm Description: } Given algorithm $A$, we: 
\begin{enumerate}
\item Divide $A$ into two \textbf{partitions}. We usually select the last element as the partition. 
\item We call the partition function to put all values less than or equal to the pivot before the pivot and all values greater than the pivot after the pivot in $\Theta(n)$ time. 
\item Then we call the partition function on the two remaining partitions recursively.
\item We end with a sorted list.
\end{enumerate}



\begin{algorithm}[H]
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\SetAlgoLined
\Input{Array $A$, partition start index $r$, partition end index $r$.} 
\Output{$A$ partitioned about $A[r]$, index of partition returned.}
	\BlankLine
	\BlankLine
	%\nlset{\color{blue}$O(n)$}Some order $n$ operation\;
	$x = A[r]$
	$i = p-1$
	\For{$j = p:r-1$}{
		\If{$A[j] \leq x$}{
			i = i+1
			exchange $A[i], A[j]$
		}
	}
	exchange $A[i+1], A[r]$
	return i+1
	\caption{Partition Subroutine for Quicksort ($\Theta(n)$)}
\end{algorithm}



\begin{algorithm}[H]
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\SetAlgoLined
\Input{Unsorted $A$, partition start index $p$, end index $r$}
\Output{Sorted $A$}
	\BlankLine
	\BlankLine
	%\nlset{\color{blue}$O(n)$}Some order $n$ operation\;
	\If{$p < r$}{
		q = Partition($A, p, r$)\;
		Quicksort($A, p, q-1$)\;
		Quicksort($A, q+1, r$)\;
	}	
	\caption{Quicksort Algorithm.}
\end{algorithm}

\paragraph{Algorithm Complexity: } $\Theta(n^2)$ worst case, $O(n\log n)$ average case with small coefficients. 


\section{Linear Time Sort}

\subsection{Counting Sort}

\paragraph{Key Idea: } If we know that the input are integers and can be used as the address in an array, we can go through one-by-one and count the number of each possible element. This requires a lot of space, though, if the range of integers is high. 

\begin{algorithm}[H]
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\SetAlgoLined
\Input{Input array $A$, output array $B$, upper bound on integers $k$}
\Output{}
	\BlankLine
	\BlankLine
	%\nlset{\color{blue}$O(n)$}Some order $n$ operation\;
	$C$ = array of zeros of size $k+1$\;

	\For{$j = 1:|A|$}{
		$C[A[j]]$++\;
	}

	\For{$i = 1:k$}{
		$C[i]$ += $C[i] + C[i-1]$\;
	}

	\For{$j = |A|:1$}{
		$B[C[A_j]] = A[j]$\;
		$C[A_j]$--\;
	}
	\caption{Counting Sort Algorithm.}
\end{algorithm}
\paragraph{Time Complexity: } $\Theta(k+n)$.

\paragraph{Stability: } The order of the entries with the same values is maintained from input to output. Relevant for \textbf{radix sort}, and for when ``satellite'' data is carried with ``keys'' used in sort.



\subsection{Radix Sort}

\paragraph{Algorithm Idea: } Start by sorting based on least significant digit then the most. This, again, only works for integer sorting or similar. We make use of a \textbf{stable sort} (usually \textbf{counting sort}). 

\begin{algorithm}[H]
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\SetAlgoLined
\Input{Integer array $A$, number of digits in each number $d$}
\Output{Sorted array $A$}
	\BlankLine
	\BlankLine
	%\nlset{\color{blue}$O(n)$}Some order $n$ operation\;
	\For{$i = 1:d$}{
		StableSort $A$ based on digit $i$\;
	}

	\caption{Radix Sorting Algorithm.}
\end{algorithm}

\paragraph{Algorithm Complexity: } $O(d(n+k))$ time complexity, assuming that StableSort is $O(n+k)$.


For $b$-bit numbers and any $r\in [0,b]$: 
$$\text{Radix Sort} = \Theta((\frac{b}{r} (n+2^r)))$$


\section{Order Statistics}

\begin{theorem}{Order Statistic Definition:}
The \textbf{Order Statistic} of set $A$ is the $i$th smallest element of the set. So,
\begin{itemize}
\item $\max A$ is the $n$th order statistic of $A$.
\item $\min A$ is the $1$st order statistic of $A$.
\item Median $A$ is the $\floor{\frac{n+1}{2}}$ order statistic. 
\end{itemize}
\end{theorem}


\subsection{Minimum and Maximum Algorithms}

Minimum and maximum algorithms just iterate through to find min or max. 

\paragraph{Finding minimum and maximum efficiently: } Iterate through, but compare \textbf{next two elements} at a time. Compare the larger element to the current maximum and the smaller of the two to the current minimum.
\begin{itemize}
\item \textbf{Even length array: } $3n/2-2$ comparisons.
\item \textbf{Odd length array: } $3\floor{n/2}$ comparisons.
\end{itemize}

\subsection{Selection in Average Linear Time}

\paragraph{Outcome: } It is possible to find the $i$th smallest element in average case $O(n)$ time (worst case $\Theta(n^2)$).

\begin{algorithm}[H]
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\SetAlgoLined
\Input{Array $A$, partition bounds $p,r$, desired order statistic $i$}
\Output{The $i$th order statistic of $A$}
	\BlankLine
	\BlankLine
	%\nlset{\color{blue}$O(n)$}Some order $n$ operation\;
	\If{$p = r$}{
		return $A[p]$\;
	}

	$q$ = RandomizedPartition($A,p,r$)\;
	$k = q-p+1$\;
	
	\eIf{$i=k$}{
		return $A[q]$\;
	}{$i < k$}{
		return RandomizedSelect($A,p,q-1,i$)\;
	}{
		return RandomizedSelect($A,q+1,r,i-k$)\;
	}
	\caption{Randomized Select Algorithm}
\end{algorithm}

\subsection{Selection in Worst Case Linear Time}

\paragraph{Outcome: } This algorithm can select the $i$th order statistic in $O(n)$ time.

\paragraph{Algorithm: } 
\begin{enumerate}
\item Partition $A$ into $\floor{n/5}$ groups of 5 and one ``extras'' group.
\item Find median of each group of find using \textbf{insertion sort}.
\item Let $x = $ \textbf{Select}(medians, $\floor{number of groups/2}$).
\item Let $k = $ \textbf{Partition}($A, 0, |A|, \text{pivot}=x$). $k$ is now the number of elements larger than $x$.
\item If $i=k$ then return $x$. Else, if $i < k$ then return \textbf{Select}($A[0:k],i$). Otherwise, return \textbf{Select}(A[k:],i-k).
\end{enumerate}


By substitution, we can show that the algorithm runs in $O(n)$.











\chapter{Hash Tables}

Perform well for tasks like \textbf{insert, search, and delete}. $\Theta(n)$ worst case for search, $O(1)$ is the practical average. 

\section{Direct Addressing}

\begin{itemize}
\item Each element we want to store has a \textbf{key} k in \textbf{universe} $U$.
\item $|U| = m$, $U = \{0, 1, ..., m-1\}$.
\item $T[0,...,m-1]$ is the \textbf{direct address table}.
\end{itemize}

\paragraph{Direct Address Search}$(T,k)$: return $T[k]$. 

\paragraph{Direct Address Insert}$(T,x)$: $T[x$.key$]=x$.

\paragraph{Direct Address Delete}$(T, x)$: $T[x.\text{key}] =$ NIL.

The problem arises when the universe $U$ gets large. This happens when you have a ton of unique elements. 


\section{Hash tables}

\begin{theorem}{Hash Function Definition:}
A hash function $h$ maps the latent universe of keys $U$ to a universe of some specific size $m$. That is,
\begin{equation}
h:U\to \{0,..., m-1\}
\end{equation}
\end{theorem}


\paragraph{Collision Resoution with Chaining: } Just add to a linked list. 
\begin{enumerate}
\item \textbf{Insertion: } $x$ is added to the \textbf{head} of the linked list.
\item \textbf{Search: } Search for element in the linked list that it hashes to.
\item \textbf{Delete: } Delete from linked list as usual.
\end{enumerate}

\paragraph{Performance: } If the number of elements $n = O(m)$, you'll get, on average, $O(1)$ performance. 

\paragraph{Simple Uniform Hashing: } Each element has same probability of hashing to given value. 


\section{Hash Functions}

Here are a few types of commonly-used hash functions. Note that most datatypes (strings, characters, etc.) can be converted to natural numbers by place value. They just might be very large. 

\subsection{Divisision Method}

To map key $k$ to one of $m$ slots, we can let 
\begin{equation}
h(k) = k \% m
\end{equation}

\paragraph{Considerations: } 
\begin{itemize}
\item $m$ should not be $2^n$ because then it'd just extract bottom $n$ bits. 
\item Usually good to pick prime number $m$.
\end{itemize}

\subsection{Multiplication Method}

We multiply $k$ by some $A\in [0,1]$ then use division method.
\begin{equation}
h(k) = \floor{m(kA\% 1)}
\end{equation}

\paragraph{Considerations: } 
\begin{itemize}
\item Value of $m$ does not matter any more ($2^n$ makes for easy hardware implementation.
\item $A \approx \frac{\sqrt 5 - 1}{2}$ tends to work well.
\end{itemize}

\subsection{Open Addressing}

The key idea is to let $h()$ take in a ``probe number'' $i$. We increment this number to make sure hash table gets filled without doubling up on a single slot. 

\begin{itemize}
\item \textbf{Probe sequence} of $k$ is $\langle h(k,0), ..., h(k, m-1) \rangle$.
\item The probe sequence for each $k$ is a \textbf{permutation of} $\langle 0,...,m-1  \rangle$.
\item Because of this property, we can always fill up the table perfectly.
\end{itemize}

\paragraph{Insertion: } Probe table, incrementing $i$ until you reach an empty slot. Add the element there. 
\paragraph{Search: } Keep probing until you either find the element \textbf{or you find NIL}. 
\paragraph{Deletion: } When you find value, set it to DELETED -- NIL would stop searches that would have to traverse. 


\paragraph{Performance of Open Addressing: } 
\begin{itemize}
\item If $n/m \leq 1$: The expected number of probes for an \textbf{unsuccessful} search is $$\gamma \leq \frac{1}{1-\frac{n}{m}}$$
\item Inserting to open address table also takes $\gamma$ probes on average.
\item Number of probes for a successful search is $$\leq \frac{1}{\alpha} \ln \frac{1}{1-\alpha}$$
Where $\alpha = n/m \leq 1$.
\end{itemize}

\paragraph{Linear Probing: } If $h'(k)$ is single-variable hash function, we can make a probing hash function via: 
\begin{equation}
h_{lin}(k,i) = (h'(k) + i)\% m
\end{equation}

\textbf{Drawback:} Long strings of filled slots build up one after another.

\paragraph{Quadratic Probing: } $h'(k)$ is a single-variable hash function:
\begin{equation}
h(k,i) = (h'(k) + c_1 i + c_2 i^2)\% m
\end{equation}

\textit{Where $c_1, c_2 \geq 0$, $i\in [m-1]$}.
\begin{itemize}
\item Better performance than linear.
\item We still struggle with \textbf{secondary clustering} -- if $h'(k_1) = h'(k_2)$, we still have the same hash sequences.  
\end{itemize}


\paragraph{Double Hashing}

\begin{equation}
h(k,i) = (h_1(k) + ih_2(k)) \% m)
\end{equation}
Now probe sequences are highly unlikely to overlap for different keys. \textbf{CONDITIONS: } 
\begin{itemize}
\item $h_2(k)$ should be ``relatively prime'' to $m$. The entire should be searched by multiples of each $h_2$. 
\item \textbf{Method 1: } $m = 2^x$, make sure $h_2(k)$ maps to an odd number.
\item \textbf{Method 2: } $m$ is prime, make sure $h_2(k)$ maps to positive inteer less than $m$.
\end{itemize}



\chapter{Dynamic Programming}

\paragraph{Motivation: } We often solve the same sub-problem many times, particularly during recursive optimization algorithms. 

\paragraph{Solution: } Memorize the intermediate optimization answers. 

\section{Elements of Dynamic Programming}

\begin{enumerate}
\item Optimal Substructure. 
\item Overlapping sub-problems. 
\end{enumerate}

\paragraph{Optimal sub-structure: } Optimal solution is $f(\text{input}, \text{optimal solutions to sub-problem})$. A common pattern is: 
\begin{enumerate}
\item Make some choice (hopefully optimal choice).
\item Realize you need to solve basically the same problem (but smaller).
\item Observer recurrence.
\item Prove \textbf{by contradiction} that optimal sub-solution leads to the optimal solution.
\end{enumerate}

\paragraph{Tips and Tricks} 
\begin{itemize}
\item Keep sub-problem space as simple as possible. 
\item Think through sequential alternatives you might try to make things countable.
\item You can try to create a sub-problem graph, too. 
\item \textbf{PRACTICE}.
\end{itemize}

\section{Examples}
\textit{High-level descriptions of dynamic programming algorithms.} It seems that most problems are variations on a smaller set of dynamic programming problems. 

\subsection{Rod Cutting}
\paragraph{Given: } A rod of length $n$ and a vector $p = [15, 20, 31, ..., 100]$ representing selling price for different lengths of rod. 

\paragraph{Wanted: } Maximum price to sell the rod for given optimal cutting.

\paragraph{Dynamic Programming Solution: } 
\begin{itemize}
\item The first step is to determine where to place your first cut. 
\item To decide between a 1-unit long cut and a $p[10]$ unit long cut, you would want to know the reward from each possible cut AND the maximum possible reward for what remains. 
\item Therefore, \begin{equation}
\text{optim}(n, p) = \max\{p_n, p_{n-1} + \text{optim}(1,p), ..., p_1+\text{optim}(n-1,p)\}
\end{equation}
\item With a general recursive strategy, we would end up calculating the same $\text{optim}(n,p)$ many time. 
\item Therefore, as soon as we calculate some $\text{optim}(n,p)$, we store it in a hash table (called \textbf{memoization}. 
\end{itemize}

\subsection{Matrix Chain Multiplication}


\subsection{Longest Common Subsequence}
%TODO: Get this together.



\subsection{Knapsack Problem}





\subsection{Huffman Codes}












\chapter{Greedy Algorithms}

\section{Introduction}

The essence of a greedy algorithm is to select the \textbf{locally optimal} decision at every opportunity. It is generally efficient, and when it is optimal, is a very convenient choice. Greedy algorithms approximate NP-complete problems well.

\section{Examples}

\subsection{Class Scheduling}

%TODO: Populate this section.







\chapter{Amortized Analysis}

\paragraph{What is amortized analysis? } 
\begin{itemize}
\item We are often interested in the \textbf{aggregate} cost over a sequence of operations.
\item There are often connections between the number of one operation and another. 
\begin{itemize}
\item For instance, you can only pop as many items as you have pushed to a stack. 
\end{itemize}
\item Amortized analysis guarantees the \textbf{average performance for the worst case} via \textit{deterministic analysis}. 
\end{itemize}


\section{Aggregate Method}
\begin{enumerate}
\item Define an $O(1)$ operation for the problem.
\item Observe the worst-case number of operations for $n$ executions of a process.
\item Divide the cost for $n$ executions by $n$ to get amortized cost.
\end{enumerate}


\paragraph{Example: Stack } 
\begin{itemize}
\item Push and pop are $O(1)$.
\item Multipop is $O(n)$ -- this is when you pop every element. 
\item Therefore, for $n$ operations, we have at most $n\cdot O(n)$, or $n$ multipops.
\item \textbf{Aggregate:} 
\begin{itemize}
\item We cannot pop more items than have been pushed.
\item The maximum number of items pushed is $n$. 
\item Therefore the average worst case cost for $n$ operations is $O(n)$, meaning that we have average $O(1)$ for each operation.
\end{itemize}
\end{itemize}




\section{Accounting Method}

\paragraph{Central Ideas:} 
\begin{itemize}
\item We charge some number of \textbf{dollars} for each operation. This is the \textbf{amortized cost}.
\item The datastructure gets those \textbf{dollars} to complete the operation.
\item We define some ``real cost'' of each operation. The exact dollar value doesn't matter, the important thing is \textbf{whether it is a function of $n$ or just constant}.
\item If our imposed \textbf{amortized cost} is more than the ``real cost'', the data structure gets \textbf{credit}.
\begin{itemize}
\item Else, the data structure \textbf{uses credit} it has stored to complete the operation.
\item \textbf{NEGATIVE BALANCE MEANS A WRONG CREDIT ALLOCATION}.
\end{itemize}
\end{itemize}






\section{Splay Trees}

\paragraph{Motivation: } We need efficient insertion, deletion, search, and sorting.
\begin{itemize}
\item If key access frequencies are known, we can use \textbf{dynamic programming}.
\item if the key frequencies are unknown, we use \textbf{splay trees}.
\end{itemize}

\textbf{Splay trees are theoretically optimal} in the amortized case. 

\paragraph{Key Idea: } We simply use a conventional BST, but introduce a \textbf{splay} operation.
\begin{itemize}
\item An unbalanced tree is \textbf{credit rich}.
\item A balanced tree is not credit rich.
\item Credit is spent to make the tree more balanced, generally.
\end{itemize}


\begin{algorithm}[H]
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\SetAlgoLined
\Input{Node $x$ of splay tree.}
\Output{Tree is splayed (re-balanced) in place.}
	\BlankLine
	\BlankLine
	%\nlset{\color{blue}$O(n)$}Some order $n$ operation\;
	\While{$x\neq \text{root}$}{
		\If{$P(x) = \text{root}$}{
			rotate $P(x)$\;
		}
		\eIf{$P(x)$ and $x$ are both (left or right) children}{
			rotate $P(P(x))$\;
			rotate $P(x)$\;
		}{
			rotate $P(x)$\;
			rotate new $P(x)$\;
		}
	
	}
	\caption{Splay Algorithm}
\end{algorithm}


\paragraph{Cost of Splay: } 
\begin{itemize}
\item $wt(x)$ is defined as the \textbf{number of nodes} in the sub-tree rooted at $x$ (descendants of $x$ including $x$).
\item $\text{rank}(x) = \ceil{\log wt(x)}$ is the \textbf{height} of sub-tree rooted at $x$.
\item \textbf{Credit Invariant: } Node $x$ has \$ $\text{rank}(x)$ in credit. 
\item Therefore, each \{zig, zig-zig, zig-zag\} costs $$3[\text{new rank}(x) - \text{old rank}(x)]$$
\item The amortized cost of \textbf{splay} is therefore $$O(\log n)$$
\end{itemize}


\paragraph{Splay Tree Operations: } 
\begin{enumerate}
\item \textbf{Search: } 
\begin{enumerate}
\item BST Search for $x$.
\item \textbf{Splay}$(x)$.
\item \textit{Cost: } $O(\log n)$
\end{enumerate}

\item \textbf{Insert: } 
\begin{enumerate}
\item BST Insert $x$ ($x$ is now a leaf).
\item \textbf{Splay}$(x)$.
\item \textit{Cost: } $O(\log n)$
\end{enumerate}

\item \textbf{Delete} 
\begin{enumerate}
\item BST Delete $x$.
\item \textbf{Splay}$(P(x))$.
\item \textit{Cost: } $O(\log n)$
\end{enumerate}

\item \textbf{Split: } If you want to create two new sub-BST's, one with all values less than $x$ and one with values greater than $x$,
\begin{enumerate}
\item Perform \textbf{Splay Search}($x$) -- $x$ is now the root.
\item Split off the right and left trees.
\item \textit{Cost: } $O(\log n)$
\end{enumerate}


\item \textbf{Join: } Basically the reverse operation of \textbf{split}. Given two splay trees, one with values $\leq x$ and one with values $> x$,
\begin{enumerate}
\item Make the $\leq x$ tree the left child of $x$, the $> x$ the right child of $x$.
\item \textbf{Credit the tree} $O(\log n)$ dollars so that $x$ satisfies the credit invariant.
\item \textit{Cost: } $O(\log n)$
\end{enumerate}
\end{enumerate}
























\end{document}
